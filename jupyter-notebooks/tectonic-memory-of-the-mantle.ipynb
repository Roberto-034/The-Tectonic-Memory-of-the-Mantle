{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "223001a2-f2f9-49d8-8297-9fe57df790e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygmt\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import os\n",
    "import shutil\n",
    "import dask\n",
    "from astropy.io import ascii as asx\n",
    "from astropy.table import Table\n",
    "import astropy\n",
    "import subprocess\n",
    "import json\n",
    "from io import StringIO\n",
    "import math\n",
    "\n",
    "import time\n",
    "\n",
    "list_of_depths = [1, 47, 109, 202, 293, 396, 487, 597, 660, 726, 798, 874, 954, 1040, 1130, 1224, 1324, 1428, 1536, 1639, 1737, 1830, 1916, 1998, 2074, 2210, 2324, 2416, 2514, 2595, 2704, 2813, 2867]\n",
    "\n",
    "#extended_list_of_depths = [2867, 2840, 2813, 2785, 2758, 2731, 2704, 2677, 2650, 2623, 2595, 2568, 2541, 2514, 2487, 2454, 2416, 2373, 2324, 2270, 2210, 2145, 2074, 1998, 1916, 1830, 1737, 1639, 1536, 1428, 1324, 1224, 1130, 1040, 954, 874, 798, 726, 660, 597, 540, 487, 439, 396, 357, 323, 293, 268, 248, 233, 217, 202, 186, 171, 155, 140, 124, 109, 93, 78, 62, 47, 31, 16, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd047d37-1f75-4761-af7b-b2fbd188a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop through all grids and re-sample them at \"sample_interval\" (string) grid spacing \n",
    "\n",
    "def resample(grids_loc, sample_interval, out_dir):\n",
    "    grds = []\n",
    "    \n",
    "    for fn in sorted(os.listdir(grids_loc)):\n",
    "        filename = os.path.join(grids_loc, fn)\n",
    "        \n",
    "        grds.append(filename)\n",
    "        \n",
    "    for infile in grds:\n",
    "        \n",
    "        infile_comps = infile.split('/')\n",
    "        \n",
    "        outfile = out_dir+'/'+infile_comps[-1]\n",
    "        \n",
    "        pygmt.grdsample(grid = infile, outgrid = outfile, spacing = sample_interval, region = 'd', verbose = 'e')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9453dc41-6fed-4692-a17d-e575b94cc5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad zeros to the front of file name depths to make all file name depths(km) 4 digits (e.g. 47 km becomes 0047 km)\n",
    "# ensures files are sorted correctly when read in for other loops over files\n",
    "\n",
    "def mantle_flow_sort(list_of_depths, source_dir, out_dir):\n",
    "    \n",
    "    str_depths = []\n",
    "    \n",
    "    test_0km = '0'\n",
    "    test_0km = test_0km.zfill(4)+'km'\n",
    "    \n",
    "    for i in list_of_depths:\n",
    "        str_dpt = str(i).zfill(4)+'km'\n",
    "        \n",
    "        str_depths.append(str_dpt)\n",
    "         \n",
    "    for filename in sorted(os.listdir(source_dir)):\n",
    "        fn = os.path.join(source_dir, filename)\n",
    "        for dpt in str_depths:\n",
    "            \n",
    "            if dpt in filename:\n",
    "                shutil.copy(fn, out_dir)\n",
    "                \n",
    "            elif test_0km in filename:\n",
    "                shutil.copy(fn, out_dir)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a0269c-be3a-4513-87a5-5016d121285e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cross-sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce44b0a1-3392-4e48-81cc-d762ef2e4ae2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8299de3a-4354-4a1d-b6a1-d1a8d6b2f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PyGMT grdtrack function\n",
    "## LOOPS OVER NETCDF FILES AND SAMPLES THEM AT POINTS DEFINED IN A SHAPEFILE --> OUTPUTS A SET OF ASCII FILES\n",
    "\n",
    "## First function runs in PyGMT, second runs in GMT as a subprocess\n",
    "\n",
    "## transect = shapefile\n",
    "## grids_loc = directory for inpit netCDF files\n",
    "## out_loc = directory to write output ASCII files to\n",
    "## list_of_depths = list of each depth being sampled\n",
    "## wave_type = string indicating which wave type is being sampled --> _pw, _sw or _b\n",
    "\n",
    "def make_transects(transect, grids_loc, out_loc, list_of_depths, wave_type):\n",
    "    \n",
    "    grds = []\n",
    "    \n",
    "    for fn in sorted(os.listdir(grids_loc)):\n",
    "        filename = os.path.join(grids_loc, fn)\n",
    "        \n",
    "        grds.append(filename)\n",
    "        \n",
    "    count = 0\n",
    "    \n",
    "    out_ext = \".txt\"\n",
    "    \n",
    "    for grd in grds:\n",
    "        \n",
    "        dpt_string = str(list_of_depths[count])\n",
    "        \n",
    "        if len(dpt_string) < 4:\n",
    "            if len(dpt_string) == 3:\n",
    "                ofn_string = '0'+dpt_string+wave_type+out_ext\n",
    "            \n",
    "            if len(dpt_string) == 2:\n",
    "                ofn_string = '00'+dpt_string+wave_type+out_ext\n",
    "            \n",
    "            if len(dpt_string) == 1:\n",
    "                ofn_string = '000'+dpt_string+wave_type+out_ext\n",
    "        else:\n",
    "            ofn_string = dpt_string+wave_type+out_ext\n",
    "            \n",
    "        \n",
    "        outfile_name = os.path.join(out_loc, ofn_string)\n",
    "        \n",
    "        pygmt.grdtrack(transect, grd, outfile = outfile_name)\n",
    "        \n",
    "        count = count + 1\n",
    "        \n",
    "\n",
    "def grdtrack_loop(pnts, grids_loc, out_loc, list_of_depths, wave_type):\n",
    "    \n",
    "    grds = []\n",
    "    \n",
    "    for fn in sorted(os.listdir(grids_loc)):\n",
    "        filename = os.path.join(grids_loc, fn)\n",
    "        \n",
    "        grds.append(filename)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    \n",
    "    for grd in grds:\n",
    "        \n",
    "        dpt_string = str(list_of_depths[count])\n",
    "        \n",
    "        if len(dpt_string) < 4:\n",
    "            if len(dpt_string) == 3:\n",
    "                ofn_string = '0'+dpt_string+wave_type+'.txt'\n",
    "            \n",
    "            if len(dpt_string) == 2:\n",
    "                ofn_string = '00'+dpt_string+wave_type+'.txt'\n",
    "            \n",
    "            if len(dpt_string) == 1:\n",
    "                ofn_string = '000'+dpt_string+wave_type+'.txt'\n",
    "        else:\n",
    "            ofn_string = dpt_string+wave_type+'.txt'\n",
    "            \n",
    "        \n",
    "        outfile = os.path.join(out_loc, ofn_string)\n",
    "        \n",
    "        gridfile = '-G'+grd\n",
    "        \n",
    "        subprocess.run(['gmt', 'grdtrack', pnts, gridfile, '>', outfile])\n",
    "        \n",
    "        count = count + 1\n",
    "\n",
    "        \n",
    "####################################################################\n",
    "####################################################################\n",
    "####################################################################\n",
    "####################################################################\n",
    "\n",
    "\n",
    "## COMBINES GRDTRACK ASCII FILES INTO ONE FILE\n",
    "## For cross-section production - After looping over all depths using grdtrack to sample along a transect, combine all transects into one ascii file\n",
    "## Finds the lowest variation of lat/lon and replaces that column with non-dimensional depth --> e.g. if sampling along a latitude line, lat is removed\n",
    "\n",
    "\n",
    "## in_dir = location of ASCII files from grdtracks function\n",
    "## out_dir = location to write the output concatenated ASCII file --> single .txt file\n",
    "## out_str = string giving the desired name of the output file in the form --> {name}.txt\n",
    "## lonlat = integer 0 or 1 to specify whether we replace lat or lon --> set to 2 if you want it to set based on variation in each column (remove the column with the least variation)\n",
    "## Generally leave lonlat set to 2\n",
    "\n",
    "def concat_ascii(in_dir, out_dir, out_str, list_of_depths, lonlat):\n",
    "    \n",
    "    table_list = []\n",
    "    \n",
    "    ascii_files = []\n",
    "    \n",
    "    for fn in sorted(os.listdir(in_dir)):\n",
    "        filename = os.path.join(in_dir, fn)\n",
    "        \n",
    "        ascii_files.append(filename)\n",
    "    \n",
    "    \n",
    "    if lonlat == 2:\n",
    "        t_table = asx.read(ascii_files[0])\n",
    "        \n",
    "        lon_var = (t_table['col1'].max()) - (t_table['col1'].min())\n",
    "        \n",
    "        lat_var = (t_table['col2'].max()) - (t_table['col2'].min())\n",
    "        \n",
    "        if abs(lon_var) > abs(lat_var):\n",
    "            lonlat = 1\n",
    "            \n",
    "        else:\n",
    "            lonlat = 0\n",
    "        \n",
    "    count = 0\n",
    "    \n",
    "    for txt in ascii_files:\n",
    "        \n",
    "        ascii_table = asx.read(txt)\n",
    "        \n",
    "        t_len = len(ascii_table)\n",
    "        \n",
    "        dpt_list = [list_of_depths[count]] * t_len\n",
    "        \n",
    "        count = count+1\n",
    "        \n",
    "        col_name = ascii_table.colnames[lonlat]\n",
    "        \n",
    "        ascii_table[col_name] = dpt_list\n",
    "        \n",
    "        table_list.append(ascii_table)\n",
    "        \n",
    "    \n",
    "    comb_table = astropy.table.vstack(table_list)\n",
    "    \n",
    "    comb_table.rename_columns(('col1', 'col2', 'col3'), ('lon', 'lat', 'vote'))\n",
    "    \n",
    "    rename_col = comb_table.colnames[lonlat]\n",
    "    comb_table.rename_column(rename_col, 'depth')\n",
    "    \n",
    "    if lonlat == 0:\n",
    "        comb_table = comb_table['lat', 'depth', 'vote']\n",
    "        \n",
    "    comb_table['depth'] = (6371. - comb_table['depth'])/6371.\n",
    "    \n",
    "    outfile_name = os.path.join(out_dir, out_str)\n",
    "    \n",
    "    asx.write(comb_table, output = outfile_name, delimiter = '\\t')\n",
    "        \n",
    "\n",
    "####################################################################\n",
    "####################################################################\n",
    "####################################################################\n",
    "####################################################################\n",
    "\n",
    "\n",
    "## FUNCTION TO RUN BOTH BLOCKMEDIAN AND SURFACE\n",
    "## Function to run in PyGMT and in GMT as a subprocess, sometime GMT provides more functionality but these are not always used\n",
    "\n",
    "def cross_section(in_dat, out_bm, out_surf, inc, reg, ct, head):\n",
    "    \n",
    "    pygmt.blockmedian(data = in_dat, outfile = out_bm, spacing = inc, region = reg, coltypes = ct, header = head)\n",
    "\n",
    "    \n",
    "    pygmt.surface(data = out_bm, outgrid = out_surf, spacing = inc, region = reg, coltypes = ct)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def gmt_cross_section(bm_infile, increment, region, coltypes, header, bm_out, surf_out, upper_boundary, lower_boundary, tension):\n",
    "    \n",
    "    subprocess.run(['gmt', 'blockmedian', bm_infile, increment, region, coltypes, header, '>', bm_out])\n",
    "    \n",
    "    subprocess.run(['gmt', 'surface', bm_out, surf_out, increment, region, upper_boundary, lower_boundary, tension])\n",
    "    \n",
    "    \n",
    "\n",
    "####################################################################\n",
    "####################################################################\n",
    "####################################################################\n",
    "####################################################################\n",
    "\n",
    "## Function to run all above functions in order to produce a cross-section netCDF file that can be plotted as a cross-section\n",
    "\n",
    "def make_cross_section(transect, list_of_depths, vm_loc, vm_trans_out, wave_type, mf_loc, mf_trans_out, vm_work_dir,\n",
    "                      mf_work_dir, region, coltypes, header, vm_upper, vm_lower, mf_upper, mf_lower):\n",
    "    \n",
    "    make_transects(transect, vm_loc, vm_trans_out, list_of_depths, wave_type)\n",
    "    \n",
    "    make_transects(transect, mf_loc, mf_trans_out, list_of_depths, '_mf')\n",
    "    \n",
    "    concat_ascii(vm_trans_out, vm_work_dir, 'tomo_concat.txt', list_of_depths, 2)\n",
    "    \n",
    "    concat_ascii(mf_trans_out, mf_work_dir, 'mf_concat.txt', list_of_depths, 2)\n",
    "    \n",
    "    vm_cs = vm_work_dir+'tomo_concat.txt'\n",
    "    \n",
    "    vm_bm = vm_work_dir+'tomo_bm.txt'\n",
    "    \n",
    "    vm_surf = '-G'+vm_work_dir+'tomo_surf.nc'\n",
    "    \n",
    "    gmt_cross_section(vm_cs, '-I0.5/0.005', region, coltypes, '-h1', vm_bm, vm_surf, vm_upper, vm_lower, '-T0')\n",
    "\n",
    "    mf_cs = mf_work_dir+'mf_concat.txt'\n",
    "    \n",
    "    mf_bm = mf_work_dir+'mf_bm.txt'\n",
    "    \n",
    "    mf_surf = '-G'+mf_work_dir+'mf_surf.nc'\n",
    "    \n",
    "    gmt_cross_section(mf_cs, '-I0.5/0.005', region, coltypes, '-h1', mf_bm, mf_surf, mf_upper, mf_lower, '-T0')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adb928c7-dcaa-4f84-9e75-48ef33adb61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOTTING CROSS-SECTIONS CREATED ABOVE BY THE \"make_cross_section\" AND PLOTTING MULTIPLE CROSS-SECTIONS AS A SUBPLOT\n",
    "\n",
    "def gmt_plot_cs(grdfile, region, projection):\n",
    "    \n",
    "    subprocess.run(['gmt', 'begin', 'map'])\n",
    "    \n",
    "    subprocess.run(['gmt', 'grdimage', grdfile, '-B', region, projection, '-Ei'])\n",
    "    \n",
    "    subprocess.run(['gmt', 'colorbar'])\n",
    "    \n",
    "    subprocess.run(['gmt', 'end', 'show'])\n",
    "\n",
    "\n",
    "def pygmt_polar_plot(grd, frame, dpi, reg, proj, cb_frame, out_fig):\n",
    "    \n",
    "    ## Initialise a figure\n",
    "    fig = pygmt.Figure()\n",
    "    \n",
    "    # add a colourmap of a gridfile (.nc) to the figure\n",
    "    fig.grdimage(grid = grd, frame = frame, dpi = dpi, region = reg, projection = proj)\n",
    "    \n",
    "    # add a colourbar to show the scaling of the colourmap\n",
    "    fig.colorbar(frame= cb_frame)\n",
    "    \n",
    "    \n",
    "    ## Save the figure\n",
    "    fig.savefig(out_fig)\n",
    "    \n",
    "    \n",
    "###################################################\n",
    "\n",
    "\n",
    "def subplots(fsize, cs_grd, cs_frame, cs_reg, cs_proj, cb_frame, yshift, line, depth_reg):\n",
    "    \n",
    "    fig = pygmt.Figure()\n",
    "    \n",
    "    fig.subplot(nrows = 2, ncols = 1, figsize = fsize)\n",
    "    \n",
    "    fig.set_panel(panel=[0,1])\n",
    "    fig.grdimage(grid = cs_grd, frame = cs_frame, region = cs_reg, dpi = 'i', projection = cs_proj)\n",
    "    fig.basemap(projection =  cs_proj, region = depth_reg, frame = ['E', 'y+lDepth(km)'])\n",
    "    fig.colorbar(frame= cb_frame)\n",
    "    \n",
    "    fig.shift_origin(yshift=yshift)\n",
    "    \n",
    "    fig.set_panel(panel=[0,0])\n",
    "    fig.basemap(region='d', projection=\"N15c\", frame=[\"af\", \"WSne\"])\n",
    "    fig.coast(shorelines=\"0.5p,black\", projection='N15c')\n",
    "    fig.plot(data = line, pen = '1.5p,red', projection='N15c')\n",
    "    \n",
    "    return(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9e72c-368e-47d6-9bd6-d0dd21832221",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Comparison maps and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fe1bfd-f10c-4ca8-95c1-27010bdde834",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb201ff-f033-4680-93d6-153c82ecee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATING COMPARISON MAPS --> COMPARES THE LOCATION OF SUBDUCTED SLABS IN BOTH TOMOGRAPHY AND THE CONVECTION MODEL AND PRODUCES A MAP DISPLAYING THE DIFFERENCE\n",
    "\n",
    "# READ IN FILES FOR TOMOGRAPHY AND CONVECTION MODEL AND CLIPS EACH TO A DESIRED LIMIT\n",
    "# I use mf_above(no slab) set to [limit, 3], mf_below(slab) set to [limit, 2], tomo_above(slab) set to [limit, 1] and tommo_below(slab) set to [limit, 4]\n",
    "# Those values provide unique sums for grdmath so that true positive = 3, false negative = 4, false positive = 6, true negative = 7\n",
    "\n",
    "def comp_map(tomo_grid, tomo_out, tomo_above, tomo_below, mf_grid, mf_out, mf_above, mf_below, comp_out):\n",
    "    \n",
    "    pygmt.grdclip(grid = tomo_grid, outgrid = tomo_out, above = tomo_above, below = tomo_below)\n",
    "    \n",
    "    pygmt.grdclip(grid = mf_grid, outgrid = mf_out, above = mf_above, below = mf_below)\n",
    "    \n",
    "    subprocess.run(['gmt', 'grdmath', tomo_out, mf_out, 'ADD', '=', comp_out])\n",
    "    \n",
    "###################################################  \n",
    "\n",
    "## Function to loop through all files (in depth order) and perform the above comparison function on them\n",
    "\n",
    "def comp_grds(mf_dir, mf_out_dir, mf_above, mf_below, tomo_dir, tomo_out_dir, tomo_above, tomo_below, out_dir, reconstruction_string):\n",
    "    \n",
    "    mf_files = []\n",
    "    \n",
    "    for file in sorted(os.listdir(mf_dir)):\n",
    "        fn = os.path.join(mf_dir, file)\n",
    "        \n",
    "        mf_files.append(fn)\n",
    "        \n",
    "    tomo_files = []\n",
    "    \n",
    "    for file in sorted(os.listdir(tomo_dir)):\n",
    "        fn = os.path.join(tomo_dir, file)\n",
    "        \n",
    "        tomo_files.append(fn)\n",
    "            \n",
    "    mf_files = mf_files[1:-1]\n",
    "    \n",
    "    tomo_files = tomo_files[1:-1]\n",
    "    \n",
    "    for x, mf_slice in enumerate(mf_files):\n",
    "        \n",
    "        infile_tomo_comps = tomo_files[x].split('/')\n",
    "        \n",
    "        depth = (infile_tomo_comps[-1])[0:6]\n",
    "        \n",
    "        comp_out = out_dir+'/'+depth+'_'+reconstruction_string+'.nc'\n",
    "        \n",
    "        tomo_out = tomo_out_dir+'/'+infile_tomo_comps[-1]\n",
    "        \n",
    "        mf_out = mf_out_dir+'/'+depth[0:4]+'_'+reconstruction_string+'.nc'\n",
    "        \n",
    "        comp_map(tomo_files[x], tomo_out, tomo_above, tomo_below, mf_slice, mf_out, mf_above, mf_below, comp_out)\n",
    "        \n",
    "        \n",
    "#####################################################\n",
    "\n",
    "## loop through grids made in above function and map them as a horizontal depth slice, label depth in the top left corner\n",
    "\n",
    "def horizontal_fig_create(grds_dir, colour_map, output_dir, file_type):\n",
    "    \n",
    "    comp_grds = []\n",
    "    \n",
    "    for file in sorted(os.listdir(grds_dir)):\n",
    "        fn = os.path.join(grds_dir, file)\n",
    "        \n",
    "        comp_grds.append(fn)\n",
    "        \n",
    "    for x, grd in enumerate(comp_grds):\n",
    "        \n",
    "        fig = pygmt.Figure()\n",
    "        \n",
    "        fig.basemap(region='d', projection=\"N15c\", frame=[\"af\", \"WSne\"])\n",
    "        \n",
    "        fig.grdimage(grid = grd, projection=\"N15c\", cmap = colour_map, interpolation = 'n+a')\n",
    "        \n",
    "        fig.coast(shorelines=\"0.5p,black\", projection='N15c')\n",
    "        \n",
    "        d_string = str(list_of_depths[x+1]) + \" km\"\n",
    "        \n",
    "        fig.text(text = d_string, position = 'TL', font = '14p')\n",
    "        \n",
    "        grd_comps = grd.split('/')\n",
    "        \n",
    "        depth = (grd_comps[-1])[0:4]\n",
    "        \n",
    "        fig_name = output_dir+'/'+depth+'_comp_map'+file_type\n",
    "        \n",
    "        fig.savefig(fig_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85d69065-7e50-4b0a-b49a-41cffdb8a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in netCDF file from above comparison grids and count the occurrences of each value, calculate comparison success from those values\n",
    "\n",
    "def comp_accuracy(comp_grid):\n",
    "    \n",
    "    comp_xrarray = xr.open_dataarray(comp_grid)\n",
    "    \n",
    "    comp_array = comp_xrarray.to_numpy()\n",
    "    \n",
    "    true_positive = np.count_nonzero(comp_array == 3)\n",
    "    false_negative = np.count_nonzero(comp_array == 4)\n",
    "    false_positive = np.count_nonzero(comp_array == 6)\n",
    "    true_negative = np.count_nonzero(comp_array == 7)\n",
    "    \n",
    "    accuracy = (true_positive + true_negative)/(true_positive + true_negative + false_positive + false_negative)\n",
    "    \n",
    "    if (true_positive+false_negative) != 0:\n",
    "        TPR = true_positive/(true_positive+false_negative)\n",
    "    elif (true_positive+false_negative) == 0:\n",
    "        TPR = 0.\n",
    "    else:\n",
    "        TPR = -99.\n",
    "    \n",
    "    TNR = true_negative/(false_positive+true_negative)\n",
    "    \n",
    "    F1_score = (2*true_positive)/(2*true_positive + false_positive + false_negative)\n",
    "    \n",
    "    precision = true_positive/(true_positive + false_positive)\n",
    "    \n",
    "    return([accuracy, TPR, TNR, F1_score, precision])\n",
    "\n",
    "##################################################\n",
    "\n",
    "## Read in folder containing comparison grids for all depths and calculate the success metrics, store in lists for plotting\n",
    "\n",
    "def comp_stats(comp_dir):\n",
    "    \n",
    "    comp_grds = []\n",
    "    \n",
    "    for file in sorted(os.listdir(comp_dir)):\n",
    "        fn = os.path.join(comp_dir, file)\n",
    "        \n",
    "        comp_grds.append(fn)\n",
    "    \n",
    "    acc_list = []\n",
    "    \n",
    "    tpr_list = []\n",
    "    \n",
    "    tnr_list = []\n",
    "    \n",
    "    f1_list = []\n",
    "    \n",
    "    precision_list = []\n",
    "    \n",
    "    for comp_grd in comp_grds:\n",
    "        \n",
    "        comp_nums = comp_accuracy(comp_grd)\n",
    "        \n",
    "        acc_list.append(comp_nums[0])\n",
    "        \n",
    "        tpr_list.append(comp_nums[1])\n",
    "        \n",
    "        tnr_list.append(comp_nums[2])\n",
    "        \n",
    "        f1_list.append(comp_nums[3])\n",
    "        \n",
    "        precision_list.append(comp_nums[4])\n",
    "        \n",
    "    return(acc_list, tpr_list, tnr_list, f1_list, precision_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924a4c10-a9de-4cdf-bf10-b79e4e5c48c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Find slab thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81d229-cbc2-429d-940e-99cd8b6bc447",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66244393-6971-43f3-b996-0ef4094a40a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For a given model type (tomography or convection model) read in the vote or temp grids for all depths and set a threshold\n",
    "## Calculate the slab area at each depth for the given threshold, grids are projected to Earth's surface so area is given as a percentage of Earth surface area\n",
    "\n",
    "def slab_area(in_dir, limit, list_of_depths, model_type):\n",
    "\n",
    "    #if model_type == 'tomo':\n",
    "    #    limit = limit\n",
    "    \n",
    "    ## Set contour limit, appending 'r' to the start gives below a given limit, rather than above\n",
    "    if model_type == 'mf':\n",
    "        limit = 'r'+str(limit)\n",
    "        \n",
    "    #else:\n",
    "    #    return('Please set \"model_type\" parameter to either \"tomo\" or \"mf\"')\n",
    "    \n",
    "    ## Create a list of filenames directing towards each horizontal depth slice map, ordered for depth\n",
    "    fn_list = []\n",
    "    for file in sorted(os.listdir(in_dir)):\n",
    "        fn = os.path.join(in_dir, file)\n",
    "        \n",
    "        fn_list.append(fn)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for fn in fn_list:\n",
    "        \n",
    "        if count == 0:\n",
    "            count = count+1\n",
    "\n",
    "        elif count == 1:\n",
    "            df_area = pygmt.grdvolume(fn, region = 'd', contour = limit)\n",
    "            df_area['depth'] = list_of_depths[count]\n",
    "            count = count + 1\n",
    "        \n",
    "        elif count < (len(list_of_depths)-1):\n",
    "            df_area_concat = pygmt.grdvolume(fn, contour = limit, region = 'd')\n",
    "            df_area_concat['depth'] = list_of_depths[count]\n",
    "                       \n",
    "            df_area = pd.concat([df_area, df_area_concat], axis=0)\n",
    "            \n",
    "            count = count + 1\n",
    "            \n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    area_list = df_area[1].tolist()\n",
    "    \n",
    "    area_percent_list = []\n",
    "    \n",
    "    for i in np.arange(0, len(area_list)):\n",
    "        \n",
    "        percent = area_list[i]*100./5.10047818418e+14\n",
    "        \n",
    "        area_percent_list.append(percent)\n",
    "        \n",
    "    return(area_percent_list)\n",
    "\n",
    "\n",
    "#################################################\n",
    "\n",
    "## Test a range of thresholds for both tomo and convection model and store the slab areas at each depth that each threshold produces\n",
    "\n",
    "def thresholds(mf_thresh, mf_dir, tomo_thresh, tomo_dir, list_of_depths):\n",
    "    \n",
    "    mf_slab_perc = []\n",
    "    \n",
    "    for mf_limit in mf_thresh:\n",
    "        \n",
    "        mf_percent_list = slab_area(mf_dir, mf_limit, list_of_depths, 'mf')\n",
    "        \n",
    "        mf_slab_perc.append(mf_percent_list)\n",
    "        \n",
    "    tomo_slab_perc = []\n",
    "    \n",
    "    for t_limit in tomo_thresh:\n",
    "        \n",
    "        tomo_percent_list = slab_area(tomo_dir, t_limit, list_of_depths, 'tomo')\n",
    "        \n",
    "        tomo_slab_perc.append(tomo_percent_list)\n",
    "    \n",
    "    comb_list = []\n",
    "    \n",
    "    hm_meandiff = []\n",
    "    \n",
    "    hm_meanperc_diff = []\n",
    "    \n",
    "    for mf_ind, mf_percent_list in enumerate(mf_slab_perc):\n",
    "        \n",
    "        hm_md_row = []\n",
    "        \n",
    "        hm_mdperc_row = []\n",
    "        \n",
    "        for t_ind, tomo_percent_list in enumerate(tomo_slab_perc):\n",
    "            \n",
    "            diff_list = []\n",
    "            perc_diff_list = []\n",
    "            for x in range(len(mf_percent_list)):\n",
    "                \n",
    "                diff = abs(mf_percent_list[x] - tomo_percent_list[x])\n",
    "                \n",
    "                diff_list.append(diff)\n",
    "                \n",
    "                perc_diff = (min(mf_percent_list[x],tomo_percent_list[x])/max(mf_percent_list[x],tomo_percent_list[x])) *100\n",
    "                \n",
    "                perc_diff_list.append(perc_diff)\n",
    "            \n",
    "            mean_diff = sum(diff_list)/len(diff_list)\n",
    "            \n",
    "            #perc_diff_list = perc_diff_list[7:24]\n",
    "            perc_diff_list = perc_diff_list[1:-1]\n",
    "            \n",
    "            meanperc_diff = sum(perc_diff_list)/len(perc_diff_list)\n",
    "            \n",
    "            comb_tup = (mf_thresh[mf_ind], tomo_thresh[t_ind], mean_diff)\n",
    "            \n",
    "            comb_list.append(comb_tup)\n",
    "            \n",
    "            hm_md_row.append(mean_diff)\n",
    "            \n",
    "            hm_mdperc_row.append(meanperc_diff)\n",
    "            \n",
    "        hm_meandiff.append(hm_md_row)\n",
    "        \n",
    "        hm_meanperc_diff.append(hm_mdperc_row)\n",
    "            \n",
    "    return(comb_list, hm_meandiff, hm_meanperc_diff)\n",
    "\n",
    "################################################################\n",
    "\n",
    "## Save lists storing thresholded slab areas for heatmap plotting in another environment that is compatible with matplotlib\n",
    "\n",
    "def save_meandiff_lists(file_name, list_to_save):\n",
    "    \n",
    "    with open(file_name, 'w') as outfile:\n",
    "        json.dump(list_to_save, outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3776f99-6314-459e-9710-4f0ff714be97",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analysis of existing/melted slabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cbad02b-5d3d-4d5a-93ea-614492fec4f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def slab_locs(slab_dir):\n",
    "    \n",
    "    fn_list = []\n",
    "    for file in sorted(os.listdir(slab_dir)):\n",
    "        fn = os.path.join(slab_dir, file)\n",
    "        \n",
    "        fn_list.append(fn)\n",
    "        \n",
    "    list_of_lines = []\n",
    "    \n",
    "    slab_file_key = []\n",
    "    \n",
    "    for fn in fn_list:\n",
    "        \n",
    "        with open(fn, 'r') as file:\n",
    "            last_line = file.readlines()[-1]\n",
    "            \n",
    "        last_line = last_line.strip()\n",
    "        \n",
    "        depth_data = list(filter(None, last_line.split(' ')))\n",
    "        \n",
    "        depth_data = [float(x) for x in depth_data]\n",
    "        \n",
    "        depth_data[0] = 6371 - (depth_data[0]/1000)\n",
    "            \n",
    "        lat = 90 - (depth_data[1] * (180/math.pi))\n",
    "        depth_data.append(lat)\n",
    "        \n",
    "        lon = depth_data[2] * (180/math.pi)\n",
    "        depth_data.append(lon)\n",
    "        \n",
    "        depth_data = [depth_data[5], depth_data[4], depth_data[0]]\n",
    "        \n",
    "        list_of_lines.append(depth_data)\n",
    "        \n",
    "        slab_file_key.append(fn)\n",
    "        \n",
    "    slab_array = np.array(list_of_lines)\n",
    "    \n",
    "    # slab_array = slab_array[np.argsort(slab_array[:, 2])]\n",
    "    \n",
    "    return(slab_array, slab_file_key)\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "def slab_exists_arrays(slab_dir, mf_grids_dir, list_of_depths, threshold):\n",
    "    \n",
    "    slab_array, slab_file_key = slab_locs(slab_dir)\n",
    "    \n",
    "    mf_grids = []\n",
    "    for file in sorted(os.listdir(mf_grids_dir)):\n",
    "        fn = os.path.join(mf_grids_dir, file)\n",
    "        \n",
    "        mf_grids.append(fn)\n",
    "        \n",
    "    mf_grids = mf_grids[1:-1]\n",
    "    \n",
    "    depth_list = list_of_depths[1:-1]\n",
    "    \n",
    "    working_list = []\n",
    "    \n",
    "    no_slab_list = []\n",
    "    \n",
    "    slab_exist_key = []\n",
    "    \n",
    "    melt_key = []\n",
    "    \n",
    "    print(len(slab_array))\n",
    "    \n",
    "    for x, slab_data in enumerate(slab_array):\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            max_depth = next(x for x, val in enumerate(depth_list) if val > slab_data[2])\n",
    "        except:\n",
    "            slab_data = slab_array[x:x+1]\n",
    "            \n",
    "            above = pygmt.grdtrack(points = slab_data, grid = mf_grids[-1])\n",
    "            alist = above.values.tolist()[0]\n",
    "            \n",
    "            if alist[-1]<=threshold:\n",
    "                working_list.append(alist)\n",
    "                slab_exist_key.append(slab_file_key[x])\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                no_slab_list.append(alist)\n",
    "                melt_key.append(slab_file_key[x])\n",
    "                continue\n",
    "            \n",
    "        min_depth = max_depth-1\n",
    "        \n",
    "        slab_data = slab_array[x:x+1]\n",
    "        \n",
    "        below = pygmt.grdtrack(points = slab_data, grid = mf_grids[max_depth])\n",
    "        \n",
    "        above = pygmt.grdtrack(points = slab_data, grid = mf_grids[min_depth])\n",
    "        \n",
    "        blist = below.values.tolist()[0]\n",
    "        \n",
    "        alist = above.values.tolist()[0]\n",
    "        \n",
    "        test_slab = len(working_list)\n",
    "        \n",
    "        test_melt = len(no_slab_list)\n",
    "        \n",
    "        if blist[-1]<=threshold:\n",
    "            working_list.append(blist)\n",
    "            slab_exist_key.append(slab_file_key[x])\n",
    "            continue\n",
    "            \n",
    "        elif alist[-1]<=threshold:\n",
    "            working_list.append(alist)\n",
    "            slab_exist_key.append(slab_file_key[x])\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            no_slab_list.append(blist)\n",
    "            melt_key.append(slab_file_key[x])\n",
    "            \n",
    "        if test_slab != len(working_list) and test_melt != len(no_slab_list):\n",
    "            print('slab and melt')\n",
    "        elif test_slab == (len(working_list)-2):\n",
    "            print('slab double up')\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "        \n",
    "    slabs_only_array = np.array(working_list)\n",
    "    slabs_only_array = slabs_only_array[np.argsort(slabs_only_array[:, 2])]\n",
    "    \n",
    "    no_slabs_array = np.array(no_slab_list)\n",
    "    no_slabs_array = no_slabs_array[np.argsort(no_slabs_array[:, 2])]\n",
    "            \n",
    "    return(slabs_only_array, no_slabs_array, slab_exist_key, melt_key)\n",
    "\n",
    "###########################################################################\n",
    "##########################################################################\n",
    "\n",
    "\n",
    "# USE THIS ONE\n",
    "def slab_exists(slab_dir, mf_grids_dir, list_of_depths, threshold):\n",
    "    \n",
    "    slab_array, slab_file_key = slab_locs(slab_dir)\n",
    "    \n",
    "    mf_grids = []\n",
    "    for file in sorted(os.listdir(mf_grids_dir)):\n",
    "        fn = os.path.join(mf_grids_dir, file)\n",
    "        \n",
    "        mf_grids.append(fn)\n",
    "        \n",
    "    mf_grids = mf_grids[1:-1]\n",
    "    \n",
    "    depth_list = list_of_depths[1:-1]\n",
    "    \n",
    "    working_list = []\n",
    "    \n",
    "    no_slab_list = []\n",
    "    \n",
    "    slab_exist_key = []\n",
    "    \n",
    "    melt_key = []\n",
    "    \n",
    "    print(len(slab_array))\n",
    "    \n",
    "    for x, slab_data in enumerate(slab_array):\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            max_depth = next(x for x, val in enumerate(depth_list) if val > slab_data[2])\n",
    "        except:\n",
    "            slab_data = slab_array[x:x+1]\n",
    "            \n",
    "            above = pygmt.grdtrack(points = slab_data, grid = mf_grids[-1])\n",
    "            alist = above.values.tolist()[0]\n",
    "            \n",
    "            if alist[-1]<=threshold:\n",
    "                working_list.append(alist)\n",
    "                slab_exist_key.append(slab_file_key[x])\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                no_slab_list.append(alist)\n",
    "                melt_key.append(slab_file_key[x])\n",
    "                continue\n",
    "            \n",
    "        min_depth = max_depth-1\n",
    "        \n",
    "        slab_data = slab_array[x:x+1]\n",
    "        \n",
    "        below = pygmt.grdtrack(points = slab_data, grid = mf_grids[max_depth])\n",
    "        \n",
    "        above = pygmt.grdtrack(points = slab_data, grid = mf_grids[min_depth])\n",
    "        \n",
    "        blist = below.values.tolist()[0]\n",
    "        \n",
    "        alist = above.values.tolist()[0]\n",
    "        \n",
    "        test_slab = len(working_list)\n",
    "        \n",
    "        test_melt = len(no_slab_list)\n",
    "        \n",
    "        if blist[-1]<=threshold:\n",
    "            working_list.append(blist)\n",
    "            slab_exist_key.append(slab_file_key[x])\n",
    "            continue\n",
    "            \n",
    "        elif alist[-1]<=threshold:\n",
    "            working_list.append(alist)\n",
    "            slab_exist_key.append(slab_file_key[x])\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            no_slab_list.append(blist)\n",
    "            melt_key.append(slab_file_key[x])\n",
    "            \n",
    "        if test_slab != len(working_list) and test_melt != len(no_slab_list):\n",
    "            print('slab and melt')\n",
    "        elif test_slab == (len(working_list)-2):\n",
    "            print('slab double up')\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "        \n",
    "    slabs_only_array = np.array(working_list)\n",
    "    slab_dict = {}\n",
    "    for i, array in enumerate(slabs_only_array):\n",
    "        slab_dict[slab_exist_key[i]] = array\n",
    "    \n",
    "    no_slabs_array = np.array(no_slab_list)\n",
    "    melt_dict = {}\n",
    "    for i, array in enumerate(no_slabs_array):\n",
    "        melt_dict[melt_key[i]] = array\n",
    "            \n",
    "    return(slabs_only_array, no_slabs_array, slab_dict, melt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a87908c5-3b06-434b-abb6-d4dbc3fb6d1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### FIND SLABS WHICH MATCH TOMOGRAPHY\n",
    "\n",
    "def slab_match_tomo(slab_dict, melt_dict, comp_dir, list_of_depths):\n",
    "    \n",
    "    comp_grds = []\n",
    "    for file in sorted(os.listdir(comp_dir)):\n",
    "        fn = os.path.join(comp_dir, file)\n",
    "        \n",
    "        comp_grds.append(fn)\n",
    "    \n",
    "    depth_list = list_of_depths[1:-1]\n",
    "    \n",
    "    slab_arrays = list(slab_dict.values())\n",
    "    slab_keys = list(slab_dict.keys())\n",
    "    \n",
    "    melt_arrays = list(melt_dict.values())\n",
    "    melt_keys = list(melt_dict.keys())\n",
    "    \n",
    "    slab_match = []\n",
    "    slab_problem = []\n",
    "    \n",
    "    melt_match = []\n",
    "    melt_problem = []\n",
    "    \n",
    "    for x, comp_grd in enumerate(comp_grds):\n",
    "        \n",
    "        working_arrays = [array for array in slab_arrays if array[2] < depth_list[x] and array[2] > depth_list[x-1]]\n",
    "        working_arrays = np.array(working_arrays)\n",
    "        \n",
    "        working_keys = [slab_keys[ind] for ind, array in enumerate(slab_arrays) if array[2] < depth_list[x] and array[2] > depth_list[x-1]]\n",
    "        \n",
    "        if len(working_arrays) != 0:\n",
    "            below_array = pygmt.grdtrack(points = working_arrays, grid = comp_grds[x-1], interpolation = 'n')\n",
    "            below_array = below_array.to_numpy()\n",
    "            above_array = pygmt.grdtrack(points = working_arrays, grid = comp_grd, interpolation = 'n')\n",
    "            above_array = above_array.to_numpy()\n",
    "        \n",
    "            for i, point in enumerate(below_array):\n",
    "                if point[-1] == 3 or point[-1] == 4:\n",
    "                    slab_match.append(working_keys[i])\n",
    "                elif above_array[i][-1] == 3 or above_array[i][-1] == 4:\n",
    "                    slab_match.append(working_keys[i])\n",
    "                else:\n",
    "                    slab_problem.append(working_keys[i])\n",
    "        \n",
    "        \n",
    "        working_arrays = [array for array in melt_arrays if array[2] < depth_list[x] and array[2] > depth_list[x-1]]\n",
    "        working_arrays = np.array(working_arrays)\n",
    "        \n",
    "        working_keys = [melt_keys[ind] for ind, array in enumerate(melt_arrays) if array[2] < depth_list[x] and array[2] > depth_list[x-1]]\n",
    "        \n",
    "        if len(working_arrays) != 0:\n",
    "            below_array = pygmt.grdtrack(points = working_arrays, grid = comp_grds[x-1], interpolation = 'n')\n",
    "            below_array = below_array.to_numpy()\n",
    "            above_array = pygmt.grdtrack(points = working_arrays, grid = comp_grd, interpolation = 'n')\n",
    "            above_array = above_array.to_numpy()\n",
    "            \n",
    "            for i, point in enumerate(below_array):\n",
    "                if point[-1] == 3 or point[-1] == 4:\n",
    "                    melt_match.append(working_keys[i])\n",
    "                elif above_array[i][-1] == 3 or above_array[i][-1] == 4:\n",
    "                    melt_match.append(working_keys[i])\n",
    "                else:\n",
    "                    melt_problem.append(working_keys[i])\n",
    "    \n",
    "    return(slab_match, slab_problem, melt_match, melt_problem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1189e6fd-cf69-48db-b031-9e4ab5039375",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FIX DICTIONARY FUNCTIONS\n",
    "\n",
    "def slab_locs(slab_dir):\n",
    "    \n",
    "    fn_list = []\n",
    "    for file in sorted(os.listdir(slab_dir)):\n",
    "        fn = os.path.join(slab_dir, file)\n",
    "        \n",
    "        fn_list.append(fn)\n",
    "        \n",
    "    list_of_lines = []\n",
    "    \n",
    "    slab_file_key = []\n",
    "    \n",
    "    for fn in fn_list:\n",
    "        \n",
    "        with open(fn, 'r') as file:\n",
    "            last_line = file.readlines()[-1]\n",
    "            \n",
    "        last_line = last_line.strip()\n",
    "        \n",
    "        depth_data = list(filter(None, last_line.split(' ')))\n",
    "        \n",
    "        # depth_data = radius, colat, lon, time\n",
    "        depth_data = [float(x) for x in depth_data]\n",
    "        \n",
    "        depth_data[0] = 6371 - (depth_data[0]/1000)\n",
    "            \n",
    "        lat = 90 - (depth_data[1] * (180/math.pi))\n",
    "        depth_data.append(lat)\n",
    "        \n",
    "        lon = depth_data[2] * (180/math.pi)\n",
    "        depth_data.append(lon)\n",
    "        \n",
    "        # depth_data = [lon, lat, depth]\n",
    "        depth_data = [depth_data[5], depth_data[4], depth_data[0]]\n",
    "        \n",
    "        list_of_lines.append(depth_data)\n",
    "        \n",
    "        slab_file_key.append(fn)\n",
    "        \n",
    "    slab_array = np.array(list_of_lines)\n",
    "    \n",
    "    # slab_array = slab_array[np.argsort(slab_array[:, 2])]\n",
    "    \n",
    "    return(slab_array, slab_file_key)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def slab_exists_weighted(slab_dir, mf_grids_dir, list_of_depths, threshold):\n",
    "    \n",
    "    slab_array, slab_file_key = slab_locs(slab_dir)\n",
    "    \n",
    "    mf_grids = []\n",
    "    for file in sorted(os.listdir(mf_grids_dir)):\n",
    "        fn = os.path.join(mf_grids_dir, file)\n",
    "        \n",
    "        mf_grids.append(fn)\n",
    "        \n",
    "    mf_grids = mf_grids[1:-1]\n",
    "    \n",
    "    depth_list = list_of_depths[1:-1]\n",
    "    \n",
    "    working_list = []\n",
    "    \n",
    "    no_slab_list = []\n",
    "    \n",
    "    slab_exist_key = []\n",
    "    \n",
    "    melt_key = []\n",
    "    \n",
    "    for x, slab_data in enumerate(slab_array):\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            max_depth = next(x for x, val in enumerate(depth_list) if val > slab_data[2])\n",
    "        except:\n",
    "            slab_data = slab_array[x:x+1]\n",
    "            \n",
    "            above = pygmt.grdtrack(points = slab_data, grid = mf_grids[-1])\n",
    "            alist = above.values.tolist()[0]\n",
    "            \n",
    "            if alist[-1]<=threshold:\n",
    "                working_list.append(alist)\n",
    "                slab_exist_key.append(slab_file_key[x])\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                no_slab_list.append(alist)\n",
    "                melt_key.append(slab_file_key[x])\n",
    "                continue\n",
    "            \n",
    "        min_depth = max_depth-1\n",
    "        \n",
    "        slab_data = slab_array[x:x+1]\n",
    "        \n",
    "        below = pygmt.grdtrack(points = slab_data, grid = mf_grids[max_depth])\n",
    "        \n",
    "        above = pygmt.grdtrack(points = slab_data, grid = mf_grids[min_depth])\n",
    "        \n",
    "        blist = below.values.tolist()[0]\n",
    "        \n",
    "        alist = above.values.tolist()[0]\n",
    "        \n",
    "        #######\n",
    "        \n",
    "        above_temp = alist[-1]\n",
    "        below_temp = blist[-1]\n",
    "        \n",
    "        above_depth = depth_list[min_depth]\n",
    "        below_depth = depth_list[max_depth]\n",
    "        point_depth = slab_data[0][2]\n",
    "        \n",
    "        weighted_anomaly = above_temp + ((point_depth - above_depth) / (below_depth - above_depth)) * (below_temp - above_temp)\n",
    "        \n",
    "        if weighted_anomaly <= threshold:\n",
    "            new_slab_array = np.array([slab_data[0][0], slab_data[0][1], slab_data[0][2], weighted_anomaly])\n",
    "            working_list.append(new_slab_array)\n",
    "            slab_exist_key.append(slab_file_key[x])\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            new_slab_array = np.array([slab_data[0][0], slab_data[0][1], slab_data[0][2], weighted_anomaly])\n",
    "            no_slab_list.append(new_slab_array)\n",
    "            melt_key.append(slab_file_key[x])\n",
    "            continue\n",
    "        \n",
    "        ######\n",
    "        \n",
    "    slabs_only_array = np.array(working_list)\n",
    "    slab_dict = {}\n",
    "    for i, array in enumerate(slabs_only_array):\n",
    "        slab_dict[slab_exist_key[i]] = array\n",
    "    \n",
    "    no_slabs_array = np.array(no_slab_list)\n",
    "    melt_dict = {}\n",
    "    for i, array in enumerate(no_slabs_array):\n",
    "        melt_dict[melt_key[i]] = array\n",
    "            \n",
    "    return(slabs_only_array, no_slabs_array, slab_dict, melt_dict)\n",
    "\n",
    "##########################################################################3\n",
    "\n",
    "def slab_match_tomo(slab_dict, melt_dict, comp_dir, list_of_depths):\n",
    "    \n",
    "    comp_grds = []\n",
    "    for file in sorted(os.listdir(comp_dir)):\n",
    "        fn = os.path.join(comp_dir, file)\n",
    "        \n",
    "        comp_grds.append(fn)\n",
    "    \n",
    "    depth_list = list_of_depths[1:-1]\n",
    "    \n",
    "    slab_arrays = list(slab_dict.values())\n",
    "    slab_keys = list(slab_dict.keys())\n",
    "    \n",
    "    melt_arrays = list(melt_dict.values())\n",
    "    melt_keys = list(melt_dict.keys())\n",
    "    \n",
    "    slab_match = []\n",
    "    slab_problem = []\n",
    "    \n",
    "    slab_match_dict = {}\n",
    "    slab_prob_dict = {}\n",
    "    \n",
    "    melt_match = []\n",
    "    melt_problem = []\n",
    "    \n",
    "    melt_match_dict = {}\n",
    "    melt_prob_dict = {}\n",
    "    \n",
    "    for x, comp_grd in enumerate(comp_grds):\n",
    "        \n",
    "        working_arrays = [array for array in slab_arrays if array[2] < depth_list[x] and array[2] > depth_list[x-1]]\n",
    "        working_arrays = np.array(working_arrays)\n",
    "        \n",
    "        working_keys = [slab_keys[ind] for ind, array in enumerate(slab_arrays) if array[2] < depth_list[x] and array[2] > depth_list[x-1]]\n",
    "        \n",
    "        if len(working_arrays) != 0:\n",
    "            below_array = pygmt.grdtrack(points = working_arrays, grid = comp_grds[x-1], interpolation = 'n')\n",
    "            below_array = below_array.to_numpy()\n",
    "            above_array = pygmt.grdtrack(points = working_arrays, grid = comp_grd, interpolation = 'n')\n",
    "            above_array = above_array.to_numpy()\n",
    "        \n",
    "            for i, point in enumerate(below_array):\n",
    "                if point[-1] == 3 or point[-1] == 4:\n",
    "                    slab_match.append(working_keys[i])\n",
    "                    slab_match_dict[working_keys[i]] = working_arrays[i]\n",
    "                elif above_array[i][-1] == 3 or above_array[i][-1] == 4:\n",
    "                    slab_match.append(working_keys[i])\n",
    "                    slab_match_dict[working_keys[i]] = working_arrays[i]\n",
    "                else:\n",
    "                    slab_problem.append(working_keys[i])\n",
    "                    slab_prob_dict[working_keys[i]] = working_arrays[i]\n",
    "        \n",
    "        \n",
    "        working_arrays = [array for array in melt_arrays if array[2] < depth_list[x] and array[2] > depth_list[x-1]]\n",
    "        working_arrays = np.array(working_arrays)\n",
    "        \n",
    "        working_keys = [melt_keys[ind] for ind, array in enumerate(melt_arrays) if array[2] < depth_list[x] and array[2] > depth_list[x-1]]\n",
    "        \n",
    "        if len(working_arrays) != 0:\n",
    "            below_array = pygmt.grdtrack(points = working_arrays, grid = comp_grds[x-1], interpolation = 'n')\n",
    "            below_array = below_array.to_numpy()\n",
    "            above_array = pygmt.grdtrack(points = working_arrays, grid = comp_grd, interpolation = 'n')\n",
    "            above_array = above_array.to_numpy()\n",
    "            \n",
    "            for i, point in enumerate(below_array):\n",
    "                if point[-1] == 3 or point[-1] == 4:\n",
    "                    melt_match.append(working_keys[i])\n",
    "                    melt_match_dict[working_keys[i]] = working_arrays[i]\n",
    "                elif above_array[i][-1] == 3 or above_array[i][-1] == 4:\n",
    "                    melt_match.append(working_keys[i])\n",
    "                    melt_match_dict[working_keys[i]] = working_arrays[i]\n",
    "                else:\n",
    "                    melt_problem.append(working_keys[i])\n",
    "                    melt_prob_dict[working_keys[i]] = working_arrays[i]\n",
    "    \n",
    "    return(slab_match_dict, slab_prob_dict, melt_match_dict, melt_prob_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12989cd2-4f24-4532-a07f-c8ede114552a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Filter slab dictionaries by depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa8aa08a-e6dc-423a-a6fb-54b05e120542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def slab_filter_depths(input_array, depth_filter_list, threshold = 100, filter_type = 0):\n",
    "    \n",
    "    list_of_depth_arrays = []\n",
    "    \n",
    "    file_at_depth_key = []\n",
    "    \n",
    "    # Group each slab terminal into groups based on the depth they are closest to\n",
    "    if filter_type == 0:\n",
    "        for x, depth in enumerate(depth_filter_list):\n",
    "            \n",
    "            if x > 0 and x != (len(depth_filter_list)-1):\n",
    "                working_array = input_array[np.logical_and((input_array[:, 2] - depth_filter_list[x-1]) > (depth - input_array[:, 2]),\n",
    "                                                           (input_array[:, 2] - depth) < (depth_filter_list[x+1] - input_array[:, 2]))]\n",
    "                list_of_depth_arrays.append(working_array)\n",
    "                \n",
    "            elif x == 0:\n",
    "                working_array = input_array[(input_array[:, 2] - depth) < ((depth_filter_list[x+1]) - input_array[:, 2])]\n",
    "                list_of_depth_arrays.append(working_array)\n",
    "                \n",
    "            else:\n",
    "                working_array = input_array[(input_array[:, 2] - depth_filter_list[x-1]) > (depth - input_array[:, 2])]\n",
    "                list_of_depth_arrays.append(working_array)\n",
    "                \n",
    "    # Create groups of all slab terminals within 'x' km of a given depth --> e.g. 3 arrays for slabs within 100km of [1000, 1500, 2000]km depths\n",
    "    else:        \n",
    "        for depth in depth_filter_list:\n",
    "            \n",
    "            working_array = input_array[abs((input_array[:, 2] - depth)) <= threshold]\n",
    "            \n",
    "            list_of_depth_arrays.append(working_array)\n",
    "            \n",
    "    return(list_of_depth_arrays)\n",
    "\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "def slab_filter_depths_dict(input_array, input_dict, depth_filter_list, threshold = 100, filter_type = 0):\n",
    "    \n",
    "    list_of_depth_dicts = []\n",
    "    \n",
    "    # Separates entries into depth which they are closest to\n",
    "    if filter_type == 0:\n",
    "        \n",
    "        dkeys = list(input_dict.keys())\n",
    "        \n",
    "        dvals = list(input_dict.values())\n",
    "        \n",
    "        for x, depth in enumerate(depth_filter_list):\n",
    "            \n",
    "            if x > 0 and x != (len(depth_filter_list)-1):\n",
    "                working_array = input_array[np.logical_and((input_array[:, 2] - depth_filter_list[x-1]) > (depth - input_array[:, 2]),\n",
    "                                                           (input_array[:, 2] - depth) < (depth_filter_list[x+1] - input_array[:, 2]))]\n",
    "                \n",
    "                \n",
    "                if len(working_array) != 0:\n",
    "                    working_array.tolist()\n",
    "                    work_dict = {}\n",
    "                    \n",
    "                    for array in working_array:\n",
    "                        dct_array_list = [x for x, d_arr in enumerate(dvals) if np.array_equal(d_arr,array) == True]\n",
    "                        \n",
    "                        work_dict[dkeys[dct_array_list[0]]] = array\n",
    "                    \n",
    "                    list_of_depth_dicts.append(work_dict)\n",
    "                    \n",
    "                else:\n",
    "                    list_of_depth_dicts.append({})\n",
    "                \n",
    "            \n",
    "            ## Append empty dictionary for first depth --> slabs seeded at 250km, first depth at 40km, should be no entries here\n",
    "            elif x == 0:\n",
    "                working_array = input_array[(input_array[:, 2] - depth) < ((depth_filter_list[x+1]) - input_array[:, 2])]\n",
    "                #list_of_depth_arrays.append(working_array)\n",
    "                list_of_depth_dicts.append({})\n",
    "            \n",
    "            \n",
    "            else:\n",
    "                working_array = input_array[(input_array[:, 2] - depth_filter_list[x-1]) > (depth - input_array[:, 2])]\n",
    "                #list_of_depth_arrays.append(working_array)\n",
    "                list_of_depth_dicts.append({})\n",
    "    \n",
    "    # Creates set of dictionaries containing slab terminals within a certain threshold of each depth in a depth list \n",
    "    # --> e.g. 3 dictionaries containing all terminals within 100km depth of [1000, 1500, 2000]km\n",
    "    else:        \n",
    "        for depth in depth_filter_list:\n",
    "            \n",
    "            work_dict = {key:array for (key,array) in input_dict.items() if abs((array[2] - depth)) <= threshold}\n",
    "            \n",
    "            list_of_depth_dicts.append(work_dict)\n",
    "                    \n",
    "    return(list_of_depth_dicts)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d518578-ab9c-41a5-8ad4-3bd023ebd504",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Slab sinking functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "239869a2-d493-4647-8f6b-6bda1d016727",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a data frame of each slabs present-day position and order them by depth\n",
    "\n",
    "#def find_slab_depths(slab_dir):\n",
    "def find_slab_present_day_depths(slab_dir):\n",
    "    \n",
    "    fn_list = []\n",
    "    for file in sorted(os.listdir(slab_dir)):\n",
    "        fn = os.path.join(slab_dir, file)\n",
    "        \n",
    "        fn_list.append(fn)\n",
    "        \n",
    "    list_of_lines = []\n",
    "    \n",
    "    for fn in fn_list:\n",
    "        \n",
    "        with open(fn, 'r') as file:\n",
    "            last_line = file.readlines()[-1]\n",
    "            \n",
    "        last_line = last_line.strip()\n",
    "        \n",
    "        split_fn = fn.split('248km/')\n",
    "        \n",
    "        depth_data = last_line+'  '+split_fn[-1]\n",
    "        \n",
    "        depth_data = list(filter(None, depth_data.split(' ')))\n",
    "        \n",
    "        list_of_lines.append(depth_data)\n",
    "    \n",
    "    for line in list_of_lines:\n",
    "        line[0] = float(line[0])\n",
    "        line[1] = float(line[1])\n",
    "        line[2] = float(line[2])\n",
    "        line[3] = float(line[3])\n",
    "    \n",
    "    df_depths = pd.DataFrame(list_of_lines)\n",
    "    df_depths.columns = ['depth', 'lat', 'long', 'time', 'name']\n",
    "    \n",
    "    df_depths_sort = df_depths.sort_values(by = 'depth')\n",
    "    return(df_depths, df_depths_sort, list_of_lines)\n",
    "\n",
    "#################################################################\n",
    "#################################################################\n",
    "\n",
    "def lateral_advection_distance(slab_path_file, dtype = 'deg', sinking_rate = False):\n",
    "    \n",
    "    list_of_lines = []\n",
    "    \n",
    "    # Create an array from the input slab file lines, convert entries from string to float\n",
    "    with open(slab_path_file, 'r') as file:\n",
    "        line_list = file.readlines()\n",
    "    \n",
    "        for line in line_list:\n",
    "            line = line.strip()\n",
    "            \n",
    "            line = list(filter(None, line.split(' ')))\n",
    "            \n",
    "            line[0] = float(line[0])\n",
    "            line[1] = float(line[1])\n",
    "            line[2] = float(line[2])\n",
    "            line[3] = float(line[3])\n",
    "            \n",
    "            list_of_lines.append(line)\n",
    "            \n",
    "    work_array = np.array(list_of_lines)\n",
    "    \n",
    "    #lat_dist = [0]\n",
    "    #depth_list = [248.]\n",
    "    lat_dist = []\n",
    "    depth_list = []\n",
    "    \n",
    "    total_lat_distance = []\n",
    "    total_vert_distance = []\n",
    "    \n",
    "    sinking_dist = []\n",
    "    \n",
    "    sinking_rate_list = []\n",
    "    \n",
    "    for x, line in enumerate(work_array):\n",
    "        \n",
    "        if x == 0:\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Set radius = midpoint between each depth\n",
    "            radius = (line[0] + work_array[x-1][0])/2\n",
    "            #radius = 6371000 # Earth Radius\n",
    "            \n",
    "            delta_d = abs(line[0] - work_array[x-1][0])\n",
    "            total_vert_distance.append(delta_d/1000)\n",
    "            \n",
    "            # Set initial latitude and longitude variables (convert colat to lat)\n",
    "            lat1 = ((90*math.pi/180) - line[1])\n",
    "            lon1 = line[2]\n",
    "            \n",
    "            # set second lat/lon variables --> convert colat to lat\n",
    "            lat2 = ((90*math.pi/180) - work_array[x-1][1])\n",
    "            lon2 = work_array[x-1][2]\n",
    "            \n",
    "            # Calculate difference between lat/lon positions\n",
    "            del_lat = lat2 - lat1\n",
    "            del_lon = lon2 - lon1\n",
    "            \n",
    "            # Calculate great circle distance the slab has travelled\n",
    "            a = (math.sin(del_lat/2)**2) + math.cos(lat1)*math.cos(lat2)*(math.sin(del_lon/2)**2)\n",
    "            \n",
    "            c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "            \n",
    "            # Calculate distance in km\n",
    "            dist = (radius*c)/1000\n",
    "            total_lat_distance.append(dist)\n",
    "            \n",
    "            # Convert distance in km to cm, divide by 100 000 (length of timestep in years) to get cm/yr\n",
    "            dist_rate = (dist*1000*100)/100000\n",
    "            \n",
    "            # convert distance rate from cm/yr to deg/myr --> rearrange circle arc length equation l = theta*r --> theta = l/r\n",
    "            # multiply radius by 100 to convert from m to cm\n",
    "            # multiply by 1 000 000 to convert from per year rate to per myr rate\n",
    "            # Convert to degrees using np.degrees()\n",
    "            dist_rate_deg = (np.degrees((dist_rate/(radius*100))))*1000000\n",
    "            \n",
    "            # Convert radius to depth (subtract radius from earth radius) and divide by 1000 to convert from m to km\n",
    "            depth_list.append((6371000-radius)/1000)\n",
    "            \n",
    "            #lat_dist.append(dist_rate)\n",
    "            if dtype == 'deg':\n",
    "                lat_dist.append(dist_rate_deg)\n",
    "            else:\n",
    "                lat_dist.append(dist_rate)\n",
    "            \n",
    "            if sinking_rate == True:\n",
    "                vert_sr = (abs(line[0] - work_array[x-1][0])/100000)*100\n",
    "                sinking_rate_list.append(vert_sr)\n",
    "            #sinking_dist.append(vert_sr)\n",
    "            \n",
    "    # Sum lateral distances for each time step to get a total lateral distance travelled\n",
    "    advection_dist = sum(total_lat_distance)\n",
    "    \n",
    "    sinking_dist = sum(total_vert_distance)\n",
    "    \n",
    "    if sinking_rate == True:\n",
    "            return(lat_dist, advection_dist, depth_list, sinking_dist, sinking_rate_list)\n",
    "    \n",
    "    return(lat_dist, advection_dist, depth_list, sinking_dist)\n",
    "        \n",
    "def displacement_list(slab_path_dir, slab_file_list = [], dtype = 'deg', sinking_rate_calc = False):\n",
    "    \n",
    "    if len(slab_file_list) == 0:\n",
    "        fn_list = []\n",
    "        for file in sorted(os.listdir(slab_path_dir)):\n",
    "            fn = os.path.join(slab_path_dir, file)\n",
    "            \n",
    "            fn_list.append(fn)\n",
    "    else:\n",
    "        fn_list = slab_file_list\n",
    "    \n",
    "    advection_dist_list = []\n",
    "    depth_lists = []\n",
    "    lat_dist_lists = []\n",
    "    sinking_dist_list = []\n",
    "    \n",
    "    vert_sink_list = []\n",
    "    \n",
    "    for fn in fn_list:\n",
    "        if sinking_rate_calc == False:\n",
    "            lat_dist_list, advection_dist, depth_list, sinking_dist = lateral_advection_distance(fn, dtype)\n",
    "            \n",
    "        else:\n",
    "            lat_dist_list, advection_dist, depth_list, sinking_dist, vert_sr_list = lateral_advection_distance(fn, dtype, sinking_rate = sinking_rate_calc)\n",
    "            vert_sink_list.append(vert_sr_list)\n",
    "        \n",
    "        advection_dist_list.append(advection_dist)\n",
    "        depth_lists.append(depth_list)\n",
    "        lat_dist_lists.append(lat_dist_list)\n",
    "        sinking_dist_list.append(sinking_dist)\n",
    "    # return(lists of lateral movement rates, list of depths at time, list of each slabs total lateral and vertical distance)\n",
    "    \n",
    "    if sinking_rate_calc == True:\n",
    "            return(lat_dist_lists, depth_lists, advection_dist_list, sinking_dist_list, vert_sink_list)\n",
    "    \n",
    "    return(lat_dist_lists, depth_lists, advection_dist_list, sinking_dist_list)\n",
    "\n",
    "\n",
    "def smooth_lat_curve(lateral_motion_rate_lists, depth_lists):\n",
    "    ## Lat motion datasets contain many measurements at inconsistent depths, this resamples each slab at common depths by finding the midpoint between\n",
    "    ## advection rates on either side of the sample depth\n",
    "    \n",
    "    ## Initialise lists to contain each slab's resampled average lateral advection rate and the depths that the slab was sampled at\n",
    "    mean_lists = []\n",
    "    sample_depth_lists = []\n",
    "    \n",
    "    ## Iterate over each slab's lateral advection rate list\n",
    "    for x, work_rate in enumerate(lateral_motion_rate_lists):\n",
    "        # find the current loop iteration's corresponding depth list\n",
    "        work_depths = depth_lists[x]\n",
    "        \n",
    "        # Initialise a count flag at 0\n",
    "        count = 0\n",
    "        \n",
    "        # define a list of depths to sample at:\n",
    "        # begin at 250km, increase in 100km increments to the greatest depth that is less than the deepest point of the slab\n",
    "        sample_depths = np.arange(250, max(work_depths), 100)\n",
    "        \n",
    "        mean_list = [] # Initialise empty list to store the averaged rates at each depth\n",
    "        \n",
    "        # Iterate over each depth to sample and find the advection rate at the depths either side of the sample depth, average them, and add them to the list\n",
    "        for i, depth in enumerate(work_depths):\n",
    "            try:\n",
    "                if depth<sample_depths[-1]:\n",
    "                    if depth<sample_depths[count] and work_depths[i+1]>sample_depths[count]:\n",
    "                        work_mean = (work_rate[i]+work_rate[i+1])/2\n",
    "                        mean_list.append(work_mean)\n",
    "                    \n",
    "                        count = count+1\n",
    "            except:\n",
    "                continue\n",
    "        mean_lists.append(mean_list)\n",
    "        sample_depth_lists.append(sample_depths)\n",
    "    \n",
    "    # Returns: mean_lists = each slab's list of resampled rates; sample_depth_lists = each mean_list corresponding list of sampled depths\n",
    "    return(mean_lists, sample_depth_lists)\n",
    "\n",
    "\n",
    "def mean_lat_advection(smoothed_lat_lists, sampled_depths_lists):\n",
    "    # Initialise list to store lateral advection rate mean for each depth\n",
    "    lat_mean = []\n",
    "    \n",
    "    # max_length = find the length of the longest list (deepest slab)\n",
    "    # deepest_list = return the list of depths for the deepest slab\n",
    "    max_length = len(max(smoothed_lat_lists, key=len))\n",
    "    deepest_list = max(sampled_depths_lists, key=len)\n",
    "    \n",
    "    # each loop creates a list of all lateral advection rates at depth 'i', then finds the mean and standard deviation\n",
    "    for i in np.arange(max_length):\n",
    "        work_mean = []\n",
    "        \n",
    "        for sub_list in smoothed_lat_lists:\n",
    "            try:\n",
    "                work_mean.append(sub_list[i])\n",
    "                \n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        mean = sum(work_mean)/len(work_mean)\n",
    "        lat_mean.append(mean)\n",
    "    \n",
    "    # Returns: lat_mean = list of the average advection rate for each depth; deepest_list = depths which have a corresponding average in lat_mean\n",
    "    return(lat_mean, deepest_list)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "######################################################################\n",
    "\n",
    "\n",
    "def slab_depth_at_time(slab_path_file, max_time):    \n",
    "    # Initialise an empty list --> this will be filled with arrays, where each array is a time step (1-line) from the slab tracking\n",
    "    list_of_lines = []\n",
    "    \n",
    "    # Open the slab path file and read the lines\n",
    "    with open(slab_path_file, 'r') as file:\n",
    "        line_list = file.readlines()\n",
    "        \n",
    "        # Iterate over the file lines --> convert from one string for the whole line into a single string for each line element and convert that to a float\n",
    "        # Array elements --> [depth(convert from radius), colat(radians), lon(radians), time before present(convert from time since tracking began)]\n",
    "        for x, line in enumerate(line_list):\n",
    "            \n",
    "            line = line.strip()\n",
    "            \n",
    "            line = list(filter(None, line.split(' ')))\n",
    "            \n",
    "            line[0] = 6371 - (float(line[0])/1000)\n",
    "            line[1] = 90 - (np.degrees(float(line[1])))\n",
    "            line[2] = np.degrees(float(line[2]))\n",
    "            line[3] = abs(max_time - float(line[3]))\n",
    "            \n",
    "            # Calculate sinking rate --> find difference in depths since the previous time step and convert to cm/yr\n",
    "            # Set first time period sinking rate to 0\n",
    "            if x != 0:\n",
    "#                sink_rate = (abs(line[0] - list_of_lines[x-1][0]) / 100000)*1000*100\n",
    "                sink_rate = ((line[0] - list_of_lines[x-1][2]) / 100000)*1000*100\n",
    "                \n",
    "                line.append(sink_rate)\n",
    "            else:\n",
    "                line.append(0.)\n",
    "                \n",
    "            apline = [line[2], line[1], line[0], line[3], line[4]]\n",
    "        \n",
    "            list_of_lines.append(apline)\n",
    "            \n",
    "    # Turn the list of lines into an array\n",
    "    work_array = np.array(list_of_lines)   \n",
    "    \n",
    "    return(work_array)\n",
    "\n",
    "\n",
    "def slabs_data(slab_path_dir, slab_file_list = []):\n",
    "    \n",
    "    # Check to see if a list of slab files has been set, if not, a directory must be set\n",
    "    if len(slab_file_list) != 0:\n",
    "        fn_list = slab_file_list\n",
    "        \n",
    "    # Create list of slab particle files from a directory containing files for tracked slabs\n",
    "    elif os.path.isdir(slab_path_dir) == True:\n",
    "        fn_list = []\n",
    "        for file in sorted(os.listdir(slab_path_dir)):\n",
    "            fn = os.path.join(slab_path_dir, file)\n",
    "            \n",
    "            fn_list.append(fn)\n",
    "    else:\n",
    "        slab_paths = np.array([])\n",
    "        return(slab_paths)\n",
    "    # Initialise empty list to create an array from\n",
    "    array_list = []\n",
    "    \n",
    "    # From the first slab particle, find the time the slabs were tracked for\n",
    "    with open(fn_list[0], 'r') as file:\n",
    "        last_line = file.readlines()[-1]\n",
    "            \n",
    "        last_line = last_line.strip()\n",
    "        \n",
    "        depth_data = list(filter(None, last_line.split(' ')))\n",
    "    \n",
    "        max_time = float(depth_data[-1])\n",
    "    \n",
    "    # create an array from each slab particle file which details --> [lon, lat, depth, time, sinking rate]\n",
    "    # Add arrays to a list of arrays\n",
    "    for fn in fn_list:\n",
    "        \n",
    "        warray = slab_depth_at_time(fn, max_time)\n",
    "        \n",
    "        array_list.append(warray)\n",
    "    \n",
    "    \n",
    "    # Create an array of arrays where each array represents the sinking profile a single slab particle\n",
    "    slab_paths = np.array(array_list)\n",
    "    \n",
    "    return(slab_paths)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b23ac70d-9348-4036-ba34-a6d2c1ccaa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comments written for depth but works for any data recorded as a time series\n",
    "\n",
    "def timeseries_mnstdv(input_array, var_ind):\n",
    "    \n",
    "    depth_lists = []\n",
    "    \n",
    "    # create a list of slab depths for each slab and create a list of lists from them\n",
    "    # j = index position of desired variable, e.g. j = 0 if depth is desired variable and depth is in the first column\n",
    "    for array in input_array:\n",
    "        depth_lists.append(((array.T)[var_ind]).tolist())\n",
    "    \n",
    "    # Zip depth list of lists --> creates a list of lists where each list shows the depth of slabs for a given age (e.g. list[0] = all slab depths at 200ma)\n",
    "    zipped_depths = list(zip(*depth_lists))\n",
    "    \n",
    "    # Sum each list of depths at time\n",
    "    mean_depths = [sum(i) for i in zipped_depths]\n",
    "    \n",
    "    # Calculate mean by dividing the sum of depths at a given time by the total number of slabs (length of depth_lists)\n",
    "    dl_length = len(depth_lists)\n",
    "    for i, depth in enumerate(mean_depths):\n",
    "        \n",
    "        mean_depths[i] = depth/dl_length\n",
    "    \n",
    "        \n",
    "    std_dev_list = []\n",
    "    \n",
    "    # Calculate standard deviation for each time period\n",
    "    for d_list in zipped_depths:\n",
    "        std_dev = np.std(d_list)\n",
    "        std_dev_list.append(std_dev)\n",
    "        \n",
    "    std_dev_upper = []\n",
    "    \n",
    "    std_dev_lower = []\n",
    "    \n",
    "    # Create list of depths for +-1 sigma at each depth\n",
    "    for x, depth in enumerate(mean_depths):\n",
    "        \n",
    "        std_dev_upper.append(depth-(std_dev_list[x]))\n",
    "        std_dev_lower.append(depth+(std_dev_list[x]))\n",
    "    \n",
    "    time_list = ((input_array[0].T)[3]).tolist()\n",
    "    \n",
    "    # Add standard deviation lists together (second reversed) to allow for plotting of a closed polygon and create time series to match\n",
    "    std_dev_poly = std_dev_upper + std_dev_lower[::-1]\n",
    "    \n",
    "    time_2 = time_list + time_list[::-1]\n",
    "    \n",
    "    return(mean_depths, std_dev_poly, time_list, time_2)\n",
    "    \n",
    "    \n",
    "##################################################################\n",
    "\n",
    "\n",
    "# sample_depths = np.arange(rnage of depths to sample)\n",
    "\n",
    "def d_vs_sr_stats(input_array, depth_ind, sr_ind):    \n",
    "    \n",
    "    test_depth = []\n",
    "    test_rate = []\n",
    "    \n",
    "    ## create list of each slab's depths and sinking rates\n",
    "    for array in input_array:\n",
    "        \n",
    "        test_depth.append(array[:,depth_ind].tolist())\n",
    "        test_rate.append(array[:,sr_ind].tolist())\n",
    "    \n",
    "    maximum_depth = max([x for x2 in test_depth for x in x2])\n",
    "    \n",
    "    sample_depths = np.arange(250, maximum_depth, 100)\n",
    "    \n",
    "    slab_ind_list = []\n",
    "    \n",
    "    # create pairs showing a depth and the first index of a pair that encompass the depth\n",
    "    for d_list in test_depth:\n",
    "        ind_list = []\n",
    "        for td in sample_depths:\n",
    "            for x, slab_depth in enumerate(d_list):\n",
    "                if x < len(d_list)-1:\n",
    "                    if slab_depth < td and d_list[x+1] > td:\n",
    "                        ind_list.append([x, td])\n",
    "                        \n",
    "                    elif slab_depth > td and d_list[x+1] < td:\n",
    "                        ind_list.append([x, td])\n",
    "    \n",
    "        slab_ind_list.append(ind_list)\n",
    "    \n",
    "    \n",
    "    # Find the average sinking rate for the sinking rates at the 2 above mentioned indices, replace index with mean sr\n",
    "    for x, sr_list in enumerate(test_rate):\n",
    "        working_mean_rate = []\n",
    "        \n",
    "        for ind_lis in slab_ind_list[x]:\n",
    "            mean = (sr_list[ind_lis[0]]+sr_list[ind_lis[0]+1])/2\n",
    "            \n",
    "            ind_lis[0] = mean\n",
    "    \n",
    "    # ZIp lists to create a pair of lists, 0 shows mean sinking rates and 1 shows a key for the depth of the corresponding sr in list 0\n",
    "    zipped_list = []\n",
    "    for x_list in slab_ind_list:\n",
    "        wl = list(zip(*x_list))\n",
    "        for x, item in enumerate(wl):\n",
    "            wl[x] = list(item)\n",
    "        \n",
    "        zipped_list.append(wl)\n",
    "    \n",
    "    \n",
    "    # sort above lists into new lists which group sinking rates for common depths\n",
    "    full_sr_list = []\n",
    "    \n",
    "    for depth in sample_depths:\n",
    "        working_sr_list = []\n",
    "        \n",
    "        for ziplist in zipped_list:\n",
    "            index_list = []\n",
    "            idp = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    idp = ziplist[1].index(depth, idp)\n",
    "                    \n",
    "                    index_list.append(idp)\n",
    "                    idp = idp+1\n",
    "                    \n",
    "                except:\n",
    "                    break\n",
    "                \n",
    "            for i in index_list:\n",
    "                working_sr_list.append(ziplist[0][i])\n",
    "                \n",
    "        full_sr_list.append(working_sr_list)\n",
    "    \n",
    "    # Find mean and std. dev for each above depth coded sinking rate list\n",
    "    mean_sr_list = []\n",
    "    \n",
    "    std_sr_list = []\n",
    "    \n",
    "    for srl in full_sr_list:\n",
    "        if len(srl) != 0:\n",
    "            mean = sum(srl)/len(srl)\n",
    "            \n",
    "            std = np.std(srl)\n",
    "            \n",
    "            mean_sr_list.append(mean)\n",
    "            std_sr_list.append(std)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    sdv_upper = []\n",
    "\n",
    "    sdv_lower = []\n",
    "    \n",
    "    # Create +- 1 sigma std dev lists for plotting of polygon\n",
    "    for x, sr in enumerate(mean_sr_list):\n",
    "        \n",
    "        sdv_upper.append(sr-(std_sr_list[x]))\n",
    "        sdv_lower.append(sr+(std_sr_list[x]))\n",
    "    \n",
    "    # Add std dev lists together (reverse second) to plot poly, make depth list to match\n",
    "    list_sd = list(sample_depths)\n",
    "    std_dev_poly = sdv_upper+sdv_lower[::-1]\n",
    "    dpt2 = list_sd + list_sd[::-1]\n",
    "    \n",
    "    return(mean_sr_list, std_dev_poly, list_sd, dpt2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4379daf4-8e9d-435b-aa7d-d94003874009",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Plotting Slab Tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f739c-a5cf-4a1d-9180-c455b2f5a62f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example code to plot the sinking paths of slab tracer particles\n",
    "\n",
    "\n",
    "fig = pygmt.Figure()\n",
    "\n",
    "fig.basemap(region='d', projection=\"N15c\", frame=[\"af\", \"WSne\"])\n",
    "fig.grdimage(grid = '/home/robby/Desktop/slab_tracking/gld446-temp-000Ma-2677km.grd', region='d', projection = 'N15c', cmap = 'gray')\n",
    "fig.coast(shorelines=\"0.5p,black\", projection='N15c')\n",
    "\n",
    "fig.grdimage(grid = '/home/robby/honours/comp/45perc_comp/comp_grds/2074_b_gld446.nc', projection = 'N15c', cmap = '/home/robby/honours/comp_palette.cpt')\n",
    "\n",
    "fig.plot(data = '/home/robby/honours/tectonic_recons/reconstructed_200.00Ma.xy', projection = 'N15c', pen = '0.5p,black')\n",
    "\n",
    "fn_list = []\n",
    "\n",
    "slab_path_dir = '/home/robby/Desktop/slab_tracking/200Ma_248km'\n",
    "\n",
    "for file in sorted(os.listdir(slab_path_dir)):\n",
    "    fn = os.path.join(slab_path_dir, file)\n",
    "    \n",
    "    fn_list.append(fn)\n",
    "\n",
    "start = time.time()\n",
    "    \n",
    "for slab_path_file in fn_list:\n",
    "    with open(slab_path_file, 'r') as file:\n",
    "        line_list = file.readlines()\n",
    "        \n",
    "        list_of_lines = []\n",
    "        for line in line_list:\n",
    "            line = line.strip()\n",
    "            \n",
    "            line = list(filter(None, line.split(' ')))\n",
    "            \n",
    "            line[0] = 6371 - (float(line[0])/1000)\n",
    "            line[1] = float(line[1])\n",
    "            line[2] = float(line[2])\n",
    "            line[3] = float(line[3])\n",
    "            \n",
    "            line.append(line[2] * (180/math.pi))\n",
    "        \n",
    "            line.append(90 - (line[1] * (180/math.pi)))\n",
    "            \n",
    "            list_of_lines.append(line)\n",
    "            \n",
    "        \n",
    "        w_array = np.array(list_of_lines)\n",
    "        \n",
    "        fig.plot(data = w_array, incols = [4,5,0], cmap = '/home/robby/Desktop/slab_tracking/depth.cpt', style = 'c0.1', region = 'd')\n",
    "        \n",
    "end = time.time()\n",
    "    \n",
    "fig.show()\n",
    "\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae677f66-4ee4-4d44-8d3d-0d543eb26c24",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Operations on tracked slabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2223d0e-8d47-4ab1-bb31-7abfa0423c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT TERMINAL LOCATIONS FOR EXISTING AND THERMALLY ASSIMILATED SLABS AT DEPTH\n",
    "\n",
    "def terminals_vs_tomo(list_of_depths, comp_dir, slabs_array, melted_array, comp_cmap, fig_dir, age_str):\n",
    "    \n",
    "    d_list = list_of_depths[1:-1]\n",
    "    \n",
    "    fn_list = []\n",
    "    for file in sorted(os.listdir(comp_dir)):\n",
    "        fn = os.path.join(comp_dir, file)\n",
    "        \n",
    "        fn_list.append(fn)\n",
    "    \n",
    "    slabs_array = slabs_array[1:-1]\n",
    "    \n",
    "    melted_array = melted_array[1:-1]\n",
    "    \n",
    "    slabs_ind = [x for x in range(len(slabs_array)) if len(slabs_array[x]) != 0]\n",
    "    \n",
    "    melted_ind = [x for x in range(len(melted_array)) if len(melted_array[x]) != 0]\n",
    "    \n",
    "    #fn_list = [fn for x, fn in enumerate(fn_list) if x in slabs_ind or x in melted_ind]\n",
    "\n",
    "    for x, comp_grd in enumerate(fn_list):\n",
    "        \n",
    "        plot_slabs = []\n",
    "        plot_melted = []\n",
    "        if x in slabs_ind:\n",
    "            \n",
    "            work_array = pygmt.grdtrack(points = slabs_array[x], grid = comp_grd, interpolation = 'n')\n",
    "            work_array = work_array.to_numpy()\n",
    "            \n",
    "            for point in work_array:\n",
    "                if point[-1] == 3 or point[-1] == 4:\n",
    "                    plot_slabs.append(point)\n",
    "\n",
    "        if x in melted_ind:\n",
    "            work_melted = pygmt.grdtrack(points = melted_array[x], grid = comp_grd, interpolation = 'n')\n",
    "            work_melted = work_melted.to_numpy()\n",
    "            \n",
    "            for point in work_melted:\n",
    "                if point[-1] == 4 or point[-1] == 3:\n",
    "                    plot_melted.append(point)\n",
    "                    \n",
    "        if len(plot_slabs) != 0 or len(plot_melted) != 0:\n",
    "            working_fig = pygmt.Figure()\n",
    "            \n",
    "            working_fig.basemap(region='d', projection=\"N15c\", frame=[\"af\", \"WSne\"])\n",
    "            working_fig.grdimage(grid = comp_grd, projection = 'N15c', cmap = comp_cmap, interpolation = 'n+a')\n",
    "            working_fig.coast(shorelines=\"0.5p,black\", projection='N15c')\n",
    "            \n",
    "            if len(plot_slabs) > 0:\n",
    "                plot_slabs = np.array(plot_slabs)\n",
    "                working_fig.plot(data = plot_slabs, projection = 'N15c', style = 'c0.15c', incols = [0,1], color = 'red')\n",
    "                \n",
    "            if len(plot_melted) > 0:\n",
    "                plot_melted = np.array(plot_melted)\n",
    "                working_fig.plot(data = plot_melted, projection = 'N15c', style = 'i0.15c', incols = [0,1], color = 'yellow')\n",
    "            \n",
    "            fig_name_str = fig_dir+'/'+str(d_list[x])+'_'+age_str+'_tomo_v_term.png'\n",
    "            \n",
    "            if os.path.exists(fig_name_str) == False:\n",
    "                working_fig.savefig(fig_name_str)\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77cd2c9b-7735-4963-bb9b-21982fff85e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT WORMS FOR TERMINALS WHICH MATCH TOMOGRAPHY\n",
    "\n",
    "def term_v_tomo_dict(in_slab_dict, comp_dir, list_of_depths, comp_cmap, fig_dir, age_str, plot_option = 'slab'):\n",
    "    \n",
    "    d_list = list_of_depths[1:-1]\n",
    "    \n",
    "    fn_list = []\n",
    "    for file in sorted(os.listdir(comp_dir)):\n",
    "        fn = os.path.join(comp_dir, file)\n",
    "        \n",
    "        fn_list.append(fn)\n",
    "    \n",
    "    #in_slab_dict = in_slab_dict[1:-1]\n",
    "    #in_melt_dict = in_melt_dict[1:-1]\n",
    "    slab_dict = in_slab_dict[1:-1]\n",
    "    \n",
    "    #if plot_option == 'melt':\n",
    "    #    slabs_ind = [x for x in range(len(in_melt_dict)) if len(in_melt_dict[x]) != 0]\n",
    "    #    slab_dict = in_melt_dict\n",
    "    #    \n",
    "    #else:\n",
    "    #    slabs_ind = [x for x in range(len(in_slab_dict)) if len(in_slab_dict[x]) != 0]\n",
    "    #    slab_dict = in_slab_dict\n",
    "        \n",
    "    slabs_ind = [x for x in range(len(slab_dict)) if len(slab_dict[x]) != 0]\n",
    "    \n",
    "    for x, comp_grd in enumerate(fn_list):\n",
    "        \n",
    "        plot_slabs = []\n",
    "        if x in slabs_ind:\n",
    "            work_dict = slab_dict[x]\n",
    "            tarray = np.array(list(work_dict.values()))\n",
    "            fn_keys = list(work_dict.keys())\n",
    "\n",
    "            work_array = pygmt.grdtrack(points = tarray, grid = comp_grd, interpolation = 'n')\n",
    "            work_array = work_array.to_numpy()\n",
    "            \n",
    "            for i, point in enumerate(work_array):\n",
    "                if point[-1] == 3 or point[-1] == 4:\n",
    "                    plot_slabs.append(fn_keys[i])\n",
    "            \n",
    "            if len(plot_slabs) != 0:\n",
    "                working_fig = pygmt.Figure()\n",
    "                \n",
    "                working_fig.basemap(region='d', projection=\"N15c\", frame=[\"af\", \"WSne\"])\n",
    "                working_fig.grdimage(grid = comp_grd, projection = 'N15c', cmap = comp_cmap, interpolation = 'n+a')\n",
    "                working_fig.coast(shorelines=\"0.5p,black\", projection='N15c')\n",
    "                \n",
    "                for slab_path_file in plot_slabs:\n",
    "                    with open(slab_path_file, 'r') as file:\n",
    "                        line_list = file.readlines()\n",
    "        \n",
    "                        list_of_lines = []\n",
    "                        for line in line_list:\n",
    "                            line = line.strip()\n",
    "                            \n",
    "                            line = list(filter(None, line.split(' ')))\n",
    "                            \n",
    "                            line[0] = 6371 - (float(line[0])/1000)\n",
    "                            line[1] = float(line[1])\n",
    "                            line[2] = float(line[2])\n",
    "                            line[3] = float(line[3])\n",
    "                            \n",
    "                            line.append(line[2] * (180/math.pi))\n",
    "                        \n",
    "                            line.append(90 - (line[1] * (180/math.pi)))\n",
    "                            \n",
    "                            list_of_lines.append(line)\n",
    "            \n",
    "        \n",
    "                        plot_array = np.array(list_of_lines)\n",
    "        \n",
    "                        working_fig.plot(data = plot_array, incols = [4,5,0], cmap = '/media/robby/arbiter/honours/colour_maps/depth.cpt', style = 'c0.1', region = 'd')\n",
    "            \n",
    "                fig_name_str = fig_dir+'/'+str(d_list[x])+'_'+age_str+'_'+plot_option+'_tracks_v_tomo.png'\n",
    "                \n",
    "                if os.path.exists(fig_name_str) == False:\n",
    "                    working_fig.savefig(fig_name_str)\n",
    "                    \n",
    "                    \n",
    "################################################################3\n",
    "\n",
    "\n",
    "def subset_term_v_tomo(list_of_depths, depth_filter, comp_dir, slabs_array, melted_array, comp_cmap, fig_dir, age_str):\n",
    "    \n",
    "    d_list = list_of_depths[1:-1]\n",
    "    \n",
    "    fn_list = []\n",
    "    for file in sorted(os.listdir(comp_dir)):\n",
    "        fn = os.path.join(comp_dir, file)\n",
    "        \n",
    "        fn_list.append(fn)\n",
    "        \n",
    "    comp_ind = [x for x, depth in enumerate(d_list) if depth in depth_filter]\n",
    "    \n",
    "    for i, x in enumerate(comp_ind):\n",
    "        \n",
    "        comp_grd = fn_list[x]\n",
    "        \n",
    "        plot_slabs = []\n",
    "        working_slab = pygmt.grdtrack(points = slabs_array[i], grid = comp_grd, interpolation = 'n')\n",
    "        working_slab = working_slab.to_numpy()\n",
    "        for point in working_slab:\n",
    "            if point[-1] == 3 or point[-1] == 4:\n",
    "                plot_slabs.append(point)\n",
    "                \n",
    "        plot_slabs = np.array(plot_slabs)\n",
    "        \n",
    "        plot_melted = []\n",
    "        working_melted = pygmt.grdtrack(points = melted_array[i], grid = comp_grd, interpolation = 'n')\n",
    "        working_melted = working_melted.to_numpy()\n",
    "        for point in working_melted:\n",
    "            if point[-1] == 3 or point[-1] == 4:\n",
    "                plot_melted.append(point)\n",
    "                \n",
    "        plot_melted = np.array(plot_melted)\n",
    "        \n",
    "        \n",
    "        lpm = len(plot_melted)\n",
    "        lps = len(plot_slabs)\n",
    "        \n",
    "        if lps+lpm != 0:\n",
    "            \n",
    "            working_fig = pygmt.Figure()\n",
    "            \n",
    "            working_fig.basemap(region='d', projection=\"N15c\", frame=[\"af\", \"WSne\"])\n",
    "            working_fig.grdimage(grid = comp_grd, projection = 'N15c', cmap = comp_cmap, interpolation = 'n+a')\n",
    "            working_fig.coast(shorelines=\"0.5p,black\", projection='N15c')\n",
    "            \n",
    "            if lps != 0:\n",
    "                working_fig.plot(data = plot_slabs, projection = 'N15c', style = 'c0.15c', incols = [0,1], color = 'red')\n",
    "            \n",
    "            if lpm != 0:\n",
    "                working_fig.plot(data = plot_melted, projection = 'N15c', style = 'i0.15c', incols = [0,1], color = 'yellow')\n",
    "            \n",
    "            fig_name_str = fig_dir+'/'+str(d_list[x])+'_'+age_str+'_tomo_v_term.png'\n",
    "                \n",
    "            if os.path.exists(fig_name_str) == False:\n",
    "                working_fig.savefig(fig_name_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5fead2d-463a-439f-9f6d-a23b192b5762",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE MAPS WHICH SHOW ALL TERMINALS MATCHING TOMOGRAPHY AT X DEPTH COLOUR CODED BY AGE\n",
    "\n",
    "def slab_terms_all_ages(all_slab_dicts, all_melt_dicts, comp_dir, list_of_depths, age_key, fig_dir, comp_cpt, age_cpt):\n",
    "    \n",
    "    ## Cut out top and bottom entries for depth list and all dictionaries\n",
    "    d_list = list_of_depths[1:-1]\n",
    "    \n",
    "    all_slab_dicts = [dct[1:-1] for dct in all_slab_dicts]\n",
    "    all_melt_dicts = [dct[1:-1] for dct in all_melt_dicts]\n",
    "    \n",
    "    comp_fn_list = []\n",
    "    for file in sorted(os.listdir(comp_dir)):\n",
    "        fn = os.path.join(comp_dir, file)\n",
    "        \n",
    "        comp_fn_list.append(fn)\n",
    "        \n",
    "    for x, comp_grid in enumerate(comp_fn_list):\n",
    "        \n",
    "        plot_slab_terms = []\n",
    "        for age, slab_dict in enumerate(all_slab_dicts):\n",
    "            if len(slab_dict[x]) != 0:\n",
    "                work_dict = slab_dict[x]\n",
    "                tarray = np.array(list(work_dict.values()))\n",
    "                fn_keys = list(work_dict.keys())\n",
    "                print(tarray)\n",
    "        \n",
    "                # check grid value for slab terminals and create new array\n",
    "                work_array = pygmt.grdtrack(points = tarray, grid = comp_grid, interpolation = 'n')\n",
    "                work_array = work_array.to_numpy()\n",
    "                \n",
    "                for point in work_array:\n",
    "                    if point[-1] == 3 or point[-1] == 4:\n",
    "                        point = np.append(point, age_key[age])\n",
    "                        plot_slab_terms.append(point)\n",
    "        \n",
    "        plot_melt_terms = []\n",
    "        for age, melt_dict in enumerate(all_melt_dicts):\n",
    "            if len(melt_dict[x]) != 0:\n",
    "                work_dict = melt_dict[x]\n",
    "                tarray = np.array(list(work_dict.values()))\n",
    "                fn_keys = list(work_dict.keys())\n",
    "        \n",
    "                # check grid value for slab terminals and create new array\n",
    "                work_array = pygmt.grdtrack(points = tarray, grid = comp_grid, interpolation = 'n')\n",
    "                work_array = work_array.to_numpy()\n",
    "                \n",
    "                for point in work_array:\n",
    "                    if point[-1] == 3 or point[-1] == 4:\n",
    "                        point = np.append(point, age_key[age])\n",
    "                        plot_melt_terms.append(point)\n",
    "                        \n",
    "        slab_flag = len(plot_slab_terms)\n",
    "        melt_flag = len(plot_melt_terms)\n",
    "\n",
    "        if slab_flag + melt_flag != 0:\n",
    "            work_fig = pygmt.Figure()\n",
    "            \n",
    "            work_fig.basemap(region='d', projection=\"N15c\", frame=[\"af\", \"WSne\"])\n",
    "            work_fig.grdimage(grid = comp_grid, cmap = comp_cpt, projection=\"N15c\", region = 'd', interpolation='n+a')\n",
    "            work_fig.coast(shorelines=\"0.5p,black\", projection='N15c')\n",
    "            \n",
    "            if slab_flag != 0:\n",
    "                work_fig.plot(data = plot_slab_terms,\n",
    "                              projection = 'N15c',\n",
    "                              style = 'c0.15c',\n",
    "                              pen = '0.25p,black',\n",
    "                              incols = [0,1,5],\n",
    "                              cmap = age_cpt)\n",
    "                \n",
    "            if melt_flag != 0:\n",
    "                work_fig.plot(data = plot_melt_terms,\n",
    "                              projection = 'N15c',\n",
    "                              style = 'i0.15c',\n",
    "                              pen = '0.25p,black',\n",
    "                              incols = [0,1,5],\n",
    "                              cmap = age_cpt)\n",
    "            \n",
    "            work_fig.colorbar(cmap = age_cpt)\n",
    "                \n",
    "            fig_name = fig_dir+'/'+str(d_list[x])+'_age_coded_terminals.png'\n",
    "            \n",
    "            work_fig.savefig(fig_name)\n",
    "            \n",
    "\n",
    "#######################################################################################\n",
    "            \n",
    "# slab_array_list = list of arrays --> each array contains the existing slabs for a slab tracking start time filtered into depth categories\n",
    "\n",
    "def terminals_age_coded(comp_dir, slab_array_list, melt_array_list, age_list, depth_filter, fig_dir):\n",
    "    \n",
    "    # Cut out top and bottom depths\n",
    "    comp_d_list = list_of_depths[1:-1]\n",
    "    \n",
    "    fn_list = []\n",
    "    for file in sorted(os.listdir(comp_dir)):\n",
    "        fn = os.path.join(comp_dir, file)\n",
    "        \n",
    "        fn_list.append(fn)\n",
    "        \n",
    "    # find indices for comparison grids which occur in depth filter\n",
    "    comp_ind = [x for x, depth in enumerate(comp_d_list) if depth in depth_filter]\n",
    "    \n",
    "    for i, d_ind in enumerate(comp_ind):\n",
    "        # List of arrays storing slabs at a given depth for each slab tracking start time, age code each one, turn into an array\n",
    "        working_slab_list = []\n",
    "        for x, at_age_array in enumerate(slab_array_list):\n",
    "            # at_age_array = array containing existing slabs at age filtered into separate arrays based on particle depth\n",
    "            # w_array = the slab particle array corresponding to the relevant comp_grd depth\n",
    "            w_array = at_age_array[i]\n",
    "            \n",
    "            append_list = [age_list[x]]*len(w_array)\n",
    "            w_array = np.c_[w_array, append_list]\n",
    "            \n",
    "            working_slab_list.append(w_array)\n",
    "        \n",
    "        working_slab_array = np.array(working_slab_list, dtype = 'object')\n",
    "        \n",
    "        # Find the slabs particles which match tomography from the above list of arrays\n",
    "        plot_exist = []\n",
    "        for points_array in working_slab_array:\n",
    "            w_array = pygmt.grdtrack(points = points_array, grid = fn_list[d_ind], interpolation = 'n')\n",
    "            w_array = w_array.to_numpy()\n",
    "            \n",
    "            for point in w_array:\n",
    "                if point[-1] == 3 or point[-1] == 4:\n",
    "                    plot_exist.append(point)\n",
    "        Plot_exist = np.array(plot_exist)            \n",
    "        ############### Perform above operations for melted slabs\n",
    "        \n",
    "        working_melt_list = []\n",
    "        for x, at_age_array in enumerate(melt_array_list):\n",
    "            w_array = at_age_array[i]\n",
    "            \n",
    "            append_list = [age_list[x]]*len(w_array)\n",
    "            w_array = np.c_[w_array, append_list]\n",
    "            \n",
    "            working_melt_list.append(w_array)\n",
    "        \n",
    "        working_melt_array = np.array(working_melt_list, dtype = 'object')\n",
    "\n",
    "        plot_melt = []\n",
    "        for points_array in working_melt_array:\n",
    "            w_array = pygmt.grdtrack(points = points_array, grid = fn_list[d_ind], interpolation = 'n')\n",
    "            w_array = w_array.to_numpy()\n",
    "            \n",
    "            for point in w_array:\n",
    "                if point[-1] == 3 or point[-1] == 4:\n",
    "                    plot_melt.append(point)\n",
    "        plot_melt = np.array(plot_melt)            \n",
    "        ###################### Plot above data\n",
    "        \n",
    "        work_fig = pygmt.Figure()\n",
    "        \n",
    "        work_fig.basemap(region='d', projection=\"N15c\", frame=[\"af\", \"WSne\"])\n",
    "\n",
    "        work_fig.grdimage(grid = fn_list[d_ind], projection = 'N15c', cmap = '/media/robby/arbiter/honours/colour_maps/comp_palette.cpt', interpolation = 'n+a')\n",
    "\n",
    "        work_fig.coast(shorelines=\"0.5p,black\", projection='N15c')\n",
    "        \n",
    "        \n",
    "        work_fig.plot(data = plot_exist, projection = 'N15c', style = 'c0.15c', incols = [0,1,4], cmap = '/media/robby/arbiter/honours/colour_maps/terminals_age.cpt')\n",
    "        work_fig.plot(data = plot_melt, projection = 'N15c', style = 'i0.15c', incols = [0,1,4], cmap = '/media/robby/arbiter/honours/colour_maps/terminals_age.cpt')\n",
    "\n",
    "        work_fig.colorbar(cmap = '/media/robby/arbiter/honours/colour_maps/terminals_age.cpt', frame = [\"a40\", 'x+l\"Slab Depth\"', 'y+lkm'])\n",
    "        \n",
    "        figure_save_str = fig_dir+'/'+str()\n",
    "        \n",
    "        work_fig.savefig(figure_save_str)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7600da0b-f359-440f-bb36-9a3c16c6b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminals_age_coded(all_slab_match_dicts, all_melt_match_dicts, comp_dir, fig_dir, reconstruction, list_of_depths, comp_cmap, age_cmap,\n",
    "                        age_key = [40,80,120,160,200,240,280], cb_truncate = None):\n",
    "    \n",
    "    slab_match_arrays = []\n",
    "    \n",
    "    for age_dict in all_slab_match_dicts:\n",
    "        working_array = np.array(list(age_dict.values()))\n",
    "        slab_match_arrays.append(working_array)\n",
    "        \n",
    "    melt_match_arrays = []\n",
    "    \n",
    "    for age_dict in all_melt_match_dicts:\n",
    "        working_array = np.array(list(age_dict.values()))\n",
    "        melt_match_arrays.append(working_array)\n",
    "        \n",
    "    filtered_slab_arrays = []\n",
    "    \n",
    "    for array in slab_match_arrays:\n",
    "        work_filter = slab_filter_depths(array, list_of_depths)\n",
    "        filtered_slab_arrays.append(work_filter)\n",
    "    \n",
    "    filtered_melt_arrays = []\n",
    "    \n",
    "    for array in melt_match_arrays:\n",
    "        work_filter = slab_filter_depths(array, list_of_depths)\n",
    "        filtered_melt_arrays.append(work_filter)\n",
    "    \n",
    "    comp_grds = []\n",
    "    for file in sorted(os.listdir(comp_dir)):\n",
    "        fn = os.path.join(comp_dir, file)\n",
    "        \n",
    "        comp_grds.append(fn)\n",
    "    \n",
    "    working_depths = list_of_depths[1:-1]\n",
    "    \n",
    "    filtered_slab_arrays = [array[1:-1] for array in filtered_slab_arrays]\n",
    "    filtered_melt_arrays = [array[1:-1] for array in filtered_melt_arrays]\n",
    "    \n",
    "    # x, i\n",
    "    for x, comp_grd in enumerate(comp_grds):\n",
    "        \n",
    "        working_slab = [age_array[x] for age_array in filtered_slab_arrays]\n",
    "        working_melt = [age_array[x] for age_array in filtered_melt_arrays]\n",
    "        \n",
    "        slab_lengths = [len(age_array[x]) for age_array in filtered_slab_arrays]\n",
    "        melt_lengths = [len(age_array[x]) for age_array in filtered_melt_arrays]\n",
    "        \n",
    "        slab_check = sum(slab_lengths)\n",
    "        melt_check = sum(melt_lengths)\n",
    "        \n",
    "        if (slab_check+melt_check) != 0:\n",
    "            working_fig = pygmt.Figure()\n",
    "            \n",
    "            working_fig.basemap(region='d', projection=\"N15c\", frame=[\"af\", \"WSne\"])\n",
    "            working_fig.grdimage(grid = comp_grd, projection = 'N15c', cmap = comp_cmap, interpolation = 'n+a')\n",
    "            working_fig.coast(shorelines=\"0.5p,black\", projection='N15c')\n",
    "            \n",
    "            work_exist = []\n",
    "            if slab_check != 0:\n",
    "                for i, array in enumerate(working_slab):\n",
    "                    if len(array) !=0:\n",
    "                        append_list = [age_key[i]]*len(array)\n",
    "                        work_plot = np.c_[array, append_list]\n",
    "                        work_exist.append(work_plot)\n",
    "                \n",
    "                plot_exist = []\n",
    "                for array in work_exist:\n",
    "                    for i in array:\n",
    "                        plot_exist.append(i)\n",
    "                plot_exist = np.array(plot_exist)\n",
    "    \n",
    "                working_fig.plot(data = plot_exist, projection = 'N15c', style = 'c0.2c', pen='0.1,black', incols = [0,1,4], cmap = age_cmap)        \n",
    "            \n",
    "            work_melt = []\n",
    "            if melt_check != 0:\n",
    "                for i, array in enumerate(working_melt):\n",
    "                    if len(array) !=0:\n",
    "                        append_list = [age_key[i]]*len(array)\n",
    "                        plot_array = np.c_[array, append_list]\n",
    "                        work_melt.append(plot_array)\n",
    "                        \n",
    "                plot_melt = []\n",
    "                for array in work_melt:\n",
    "                    for i in array:\n",
    "                        plot_melt.append(i)\n",
    "                plot_melt = np.array(plot_melt)\n",
    "                        \n",
    "                working_fig.plot(data = plot_melt, projection = 'N15c', style = 'i0.2c', pen='0.1,black', incols = [0,1,4], cmap = age_cmap)\n",
    "            \n",
    "            if cb_truncate == None:\n",
    "                working_fig.colorbar(cmap = age_cmap, frame = [\"a40\", 'x+l\"age\"', 'y+lmyr'])\n",
    "            elif cb_truncate == True:\n",
    "                working_fig.colorbar(cmap = age_cmap, frame = [\"a40\", 'x+l\"age\"', 'y+lmyr'], truncate = [40,160])\n",
    "                \n",
    "            fig_name = fig_dir+'/'+reconstruction+'_'+str(working_depths[x])+'_age_coded_terminals.png'\n",
    "            \n",
    "            working_fig.savefig(fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4aae23f6-a064-4e91-8ccb-55731fe3ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE LIST OF SLABS MATCHING TOMOGRAPHY TO BE PLOTTED AS WORMS \n",
    "\n",
    "def working_slab_tracks(in_slab_dict, in_melt_dict, comp_dir):\n",
    "    \n",
    "    slab_dict_list = in_slab_dict[1:-1]\n",
    "    melt_dict_list = in_melt_dict[1:-1]\n",
    "    \n",
    "    comp_fn_list = []\n",
    "    for file in sorted(os.listdir(comp_dir)):\n",
    "        fn = os.path.join(comp_dir, file)\n",
    "        \n",
    "        comp_fn_list.append(fn)\n",
    "    \n",
    "    plot_slab_worms = []\n",
    "    plot_melt_worms = []\n",
    "    for x, comp_grd in enumerate(comp_fn_list):\n",
    "        \n",
    "        if len(slab_dict_list[x]) != 0:\n",
    "            ## Grab dictionary corresponding to comp depth, take the slab data and create an array from them, get list of file names for these slabs\n",
    "            work_dict = slab_dict_list[x]\n",
    "            tarray = np.array(list(work_dict.values()))\n",
    "            fn_keys = list(work_dict.keys())\n",
    "            \n",
    "            # check grid value for slab terminals and create new array\n",
    "            work_array = pygmt.grdtrack(points = tarray, grid = comp_grd, interpolation = 'n')\n",
    "            work_array = work_array.to_numpy()\n",
    "            \n",
    "            working_points = []\n",
    "            # if a terminal sits over a tomo slab, add corresponding file to list of slabs to be plotted\n",
    "            for i, point in enumerate(work_array):\n",
    "                if point[-1] == 3 or point[-1] == 4:\n",
    "                    working_points.append(fn_keys[i])\n",
    "                    \n",
    "            plot_slab_worms.append(working_points)\n",
    "                \n",
    "        else:\n",
    "            plot_slab_worms.append([])\n",
    "            \n",
    "        if len(melt_dict_list[x]) != 0:\n",
    "            ## Grab dictionary corresponding to comp depth, take the slab data and create an array from them, get list of file names for these slabs\n",
    "            work_dict = melt_dict_list[x]\n",
    "            tarray = np.array(list(work_dict.values()))\n",
    "            fn_keys = list(work_dict.keys())\n",
    "            \n",
    "            # check grid value for slab terminals and create new array\n",
    "            work_array = pygmt.grdtrack(points = tarray, grid = comp_grd, interpolation = 'n')\n",
    "            work_array = work_array.to_numpy()\n",
    "            \n",
    "            working_points = []\n",
    "            # if a terminal sits over a tomo slab, add corresponding file to list of slabs to be plotted\n",
    "            for i, point in enumerate(work_array):\n",
    "                if point[-1] == 3 or point[-1] == 4:\n",
    "                    working_points.append(fn_keys[i])\n",
    "                    \n",
    "            plot_melt_worms.append(working_points)\n",
    "            \n",
    "        else:\n",
    "            plot_melt_worms.append([])\n",
    "            \n",
    "    return(plot_slab_worms, plot_melt_worms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36b08fe9-4be8-46bb-bf07-58d629c4532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot slabs vs subduction zones based on above/below checks\n",
    "\n",
    "def plot_initial_slabs(list_of_slab_files):\n",
    "    \n",
    "    plot_initial_arrays = []\n",
    "    \n",
    "    # Iterate over lists of slab files\n",
    "    if isinstance(list_of_slab_files[0], list):\n",
    "        for slab_file_list in list_of_slab_files:\n",
    "            \n",
    "            working_list = []\n",
    "            \n",
    "            # Iterate over slab track files in slab files list\n",
    "            for file in slab_file_list:\n",
    "                apline = []\n",
    "                # Open file and store the first and last lines\n",
    "                with open(file, 'r') as slab_file:\n",
    "                    lines = slab_file.readlines()\n",
    "                    first_line = lines[0]\n",
    "                    last_line = lines[-1]\n",
    "                    \n",
    "                    first_line = first_line.strip()\n",
    "                    first_line = list(filter(None, first_line.split(' ')))\n",
    "                    \n",
    "                    last_line = last_line.strip()\n",
    "                    last_line = list(filter(None, last_line.split(' ')))\n",
    "                    \n",
    "                    first_line = [float(x) for x in first_line]\n",
    "                    \n",
    "                    # Create line containing [initial_lon, initial_lat, terminal_depth]\n",
    "                    apline.append(first_line[2] * 180/math.pi)\n",
    "                    apline.append(90 - (first_line[1] * (180/math.pi)))\n",
    "                    apline.append(6371 - (float(last_line[0])/1000))\n",
    "                    \n",
    "                    working_list.append(apline)\n",
    "            \n",
    "            # Create array that shows the start location and end depth for each slab in the file list\n",
    "            plot_initial_arrays.append(np.array(working_list))\n",
    "        \n",
    "    else:\n",
    "        slab_file_list = list_of_slab_files\n",
    "        working_list = []\n",
    "        \n",
    "        # Iterate over slab track files in slab files list\n",
    "        for file in slab_file_list:\n",
    "            apline = []\n",
    "            \n",
    "            # Open file and store the first and last lines\n",
    "            with open(file, 'r') as slab_file:\n",
    "                lines = slab_file.readlines()\n",
    "                first_line = lines[0]\n",
    "                last_line = lines[-1]\n",
    "                \n",
    "                first_line = first_line.strip()\n",
    "                first_line = list(filter(None, first_line.split(' ')))\n",
    "                \n",
    "                last_line = last_line.strip()\n",
    "                last_line = list(filter(None, last_line.split(' ')))\n",
    "                \n",
    "                first_line = [float(x) for x in first_line]\n",
    "                \n",
    "                # Create line containing [initial_lon, initial_lat, terminal_depth]\n",
    "                apline.append(first_line[2] * 180/math.pi)\n",
    "                apline.append(90 - (first_line[1] * (180/math.pi)))\n",
    "                apline.append(6371 - (float(last_line[0])/1000))\n",
    "                \n",
    "                working_list.append(apline)\n",
    "        \n",
    "        # Create array that shows the start location and end depth for each slab in the file list\n",
    "        plot_initial_arrays.append(np.array(working_list))\n",
    "    return(plot_initial_arrays)\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "\n",
    "### PLOT SLABS VS SUBDUCTION ZONES USING NEAREST DEPTH CHECK\n",
    "\n",
    "def slabs_vs_subzones(in_slab_dict, in_melt_dict, comp_dir, list_of_depths):\n",
    "    \n",
    "    d_list = list_of_depths[1:-1]\n",
    "    slab_dict = in_slab_dict[1:-1]\n",
    "    melt_dict = in_melt_dict[1:-1]\n",
    "    \n",
    "    fn_list = []\n",
    "    for file in sorted(os.listdir(comp_dir)):\n",
    "        fn = os.path.join(comp_dir, file)\n",
    "        \n",
    "        fn_list.append(fn)\n",
    "\n",
    "    slabs_ind = [x for x, dct in enumerate(slab_dict) if len(dct) != 0]\n",
    "    melt_ind = [x for x, dct in enumerate(melt_dict) if len(dct) != 0]\n",
    "    \n",
    "    plot_slabs = []\n",
    "    plot_problem_slabs = []\n",
    "    \n",
    "    plot_melt = []\n",
    "    plot_problem_melt = []\n",
    "    for x, comp_grd in enumerate(fn_list):\n",
    "        \n",
    "        if x in slabs_ind:\n",
    "            ## Grab dictionary corresponding to comp depth, take the slab data and create an array from them, get list of file names for these slabs\n",
    "            work_dict = slab_dict[x]\n",
    "            tarray = np.array(list(work_dict.values()))\n",
    "            fn_keys = list(work_dict.keys())\n",
    "            \n",
    "            # check grid value for slab terminals and create new array\n",
    "            work_array = pygmt.grdtrack(points = tarray, grid = comp_grd, interpolation = 'n')\n",
    "            work_array = work_array.to_numpy()\n",
    "            \n",
    "            # if a terminal sits over a tomo slab, add corresponding file to list of slabs to be plotted\n",
    "            for i, point in enumerate(work_array):\n",
    "                if point[-1] == 3 or point[-1] == 4:\n",
    "                    plot_slabs.append(fn_keys[i])\n",
    "                else:\n",
    "                    plot_problem_slabs.append(fn_keys[i])\n",
    "                            \n",
    "        if x in melt_ind:\n",
    "            ## Grab dictionary corresponding to comp depth, take the slab data and create an array from them, get list of file names for these slabs\n",
    "            work_dict = melt_dict[x]\n",
    "            tarray = np.array(list(work_dict.values()))\n",
    "            fn_keys = list(work_dict.keys())\n",
    "            \n",
    "            # check grid value for slab terminals and create new array\n",
    "            work_array = pygmt.grdtrack(points = tarray, grid = comp_grd, interpolation = 'n')\n",
    "            work_array = work_array.to_numpy()\n",
    "            \n",
    "            # if a terminal sits over a tomo slab, add corresponding file to list of slabs to be plotted\n",
    "            for i, point in enumerate(work_array):\n",
    "                if point[-1] == 3 or point[-1] == 4:\n",
    "                    plot_melt.append(fn_keys[i])\n",
    "                else:\n",
    "                    plot_problem_melt.append(fn_keys[i])\n",
    "                    \n",
    "    plot_sub_zones = [plot_slabs, plot_problem_slabs, plot_melt, plot_problem_melt]\n",
    "    \n",
    "    ## iterate through each list of files, grab the first line and format as 'lon,lat,depth'\n",
    "    ## add each slab initial to a new list for that category (slab/melt, problems), append that list to a list of lists\n",
    "    plot_initial_points = []\n",
    "    for file_list in plot_sub_zones:\n",
    "        \n",
    "        working_list = []\n",
    "        for file in file_list:\n",
    "            apline = []\n",
    "            with open(file, 'r') as slab_file:\n",
    "                lines = slab_file.readlines()\n",
    "                first_line = lines[0]\n",
    "                last_line = lines[-1]\n",
    "                \n",
    "                first_line = first_line.strip()\n",
    "                first_line = list(filter(None, first_line.split(' ')))\n",
    "                \n",
    "                last_line = last_line.strip()\n",
    "                last_line = list(filter(None, last_line.split(' ')))\n",
    "                \n",
    "                first_line = [float(x) for x in first_line]\n",
    "                \n",
    "                apline.append(first_line[2] * 180/math.pi)\n",
    "                apline.append(90 - (first_line[1] * (180/math.pi)))\n",
    "                apline.append(6371 - (float(last_line[0])/1000))\n",
    "                \n",
    "                working_list.append(apline)\n",
    "                \n",
    "        working_array = np.array(working_list)\n",
    "        plot_initial_points.append(working_array)\n",
    "    \n",
    "    return(plot_initial_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e0e2b5-4fbe-4259-b5a0-3f4280dc439a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AOU Slab Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb6714-1dc3-4882-8c31-e7d0e93b2304",
   "metadata": {},
   "outputs": [],
   "source": [
    "## READS IN Atlas of the Underworld DATA AND EXTRACTS THE RELEVANT DATA\n",
    "\n",
    "# columns = depth, lon, lat, base_depth, error, top_depth, error\n",
    "# first 3 columns = midpoint data\n",
    "\n",
    "aou_array_pd = (pd.read_csv('/media/robby/arbiter/honours/others_data/atlas_of_the_underworld/aou_data.csv')).to_numpy()\n",
    "\n",
    "#np_test = df_import.to_numpy()\n",
    "\n",
    "aou_array_full = np.delete(aou_array_pd, [0,1,2], axis = 0)\n",
    "\n",
    "keep_rows = [1,4,5,6,9,10,11,12]\n",
    "\n",
    "aou_array = aou_array_full[:,keep_rows]\n",
    "\n",
    "slab_name_list = list(aou_array[:,0])\n",
    "\n",
    "aou_array = np.delete(aou_array,0,axis = 1)\n",
    "aou_array = aou_array.astype(float)\n",
    "\n",
    "print(len(aou_array))\n",
    "\n",
    "keep_bases = [13,9,14,10]\n",
    "keep_tops = [15,11,16,12]\n",
    "\n",
    "aou_bases_str = aou_array_full[:,keep_bases]\n",
    "aou_tops_str = aou_array_full[:,keep_tops]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46d696a6-f6b7-4052-be38-81e71f140644",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATES ARRAYS OF AOU SLAB BASE AND TOP DATA\n",
    "\n",
    "#aou_bases = aou_bases.astype(float)\n",
    "#aou_tops = aou_tops.astype(float)\n",
    "\n",
    "work_aou_bases = []\n",
    "for array in aou_bases_str:\n",
    "    try:\n",
    "        warray = array.astype(float)\n",
    "        if warray[0] < 4.5e3:\n",
    "            if np.all(warray==0) == False:\n",
    "                \n",
    "                age = (warray[0] + warray[2])/2\n",
    "                error = abs(age - warray[0])\n",
    "                \n",
    "                warray[0] = age\n",
    "                warray[2] = error\n",
    "                work_aou_bases.append(warray)\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "aou_bases = np.array(work_aou_bases)\n",
    "\n",
    "work_aou_tops = []\n",
    "for array in aou_tops_str:\n",
    "    try:\n",
    "        warray = array.astype(float)\n",
    "        if warray[0] < 4.5e3:\n",
    "            if np.all(warray==0) == False:\n",
    "                \n",
    "                age = (warray[0] + warray[2])/2\n",
    "                error = abs(age - warray[0])\n",
    "                \n",
    "                warray[0] = age\n",
    "                warray[2] = error\n",
    "                work_aou_tops.append(warray)\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "aou_tops = np.array(work_aou_tops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "880c1243-3d92-4beb-a4c9-ce088e12f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate great circle distance between two points --> default radius is along the earths surface, optionally input radius variable\n",
    "## Input lat/lon is in degrees\n",
    "\n",
    "def earth_surf_dist(lat1, lon1, lat2, lon2, radius = 6371000):\n",
    "    \n",
    "    # define each lat points and convert to radians\n",
    "    lat1 = np.radians(lat1)\n",
    "    lat2 = np.radians(lat2)\n",
    "    \n",
    "    # Define each lon point and convert to radians\n",
    "    lon1 = np.radians(lon1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    \n",
    "    # Calculate difference between start and end lat/lon points\n",
    "    del_lat = lat2 - lat1\n",
    "    del_lon = lon2 - lon1\n",
    "    \n",
    "    a = (math.sin(del_lat/2)**2) + math.cos(lat1)*math.cos(lat2)*(math.sin(del_lon/2)**2)\n",
    "        \n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    \n",
    "    # Calculate distance and convert from m to km\n",
    "    dist = (radius*c)/1000\n",
    "    \n",
    "    # return(great circle distance between two points)\n",
    "    return(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35660a4b-a4af-4103-8be7-ee64640b19ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aou_slab_finder(list_of_depths, comp_dir, aou_name_keys, aou_slab_array, depth_range, reg_buffer, search_rad):\n",
    "    \n",
    "    # Cut out top and bottom depths from list of depths\n",
    "    d_list = list_of_depths[1:-1]\n",
    "    \n",
    "    # Create list of comparison grid files\n",
    "    grd_list = []\n",
    "    for file in sorted(os.listdir(comp_dir)):\n",
    "        fn = os.path.join(comp_dir, file)\n",
    "        \n",
    "        grd_list.append(fn)\n",
    "    \n",
    "    # initialise empty dictionary to store aou slab search data\n",
    "    aou_dict = {}\n",
    "    \n",
    "    # Iterate over slab points in aou data\n",
    "    for x, aou_slab in enumerate(aou_slab_array):\n",
    "        \n",
    "        # Find index of all comparison grids within a depth tolerance of the aou slab midpoint\n",
    "        #search_grids = [i for i, depth in enumerate(d_list) if abs(aou_slab[0] - depth) < depth_range]\n",
    "        search_grids = [i for i, depth in enumerate(d_list) if aou_slab[5] <= depth <= aou_slab[3]]\n",
    "        \n",
    "        working_list = []\n",
    "        \n",
    "        # Iterate over grids within depth tolerance\n",
    "        for grid_ind in search_grids:\n",
    "            \n",
    "            search_grid = grd_list[grid_ind]\n",
    "            \n",
    "            # Set search region --> take aou slab lat/lon as centrepoint of a square region with side lengths 2*reg_buffer(degrees, reg_buffer on either side of centre point) \n",
    "            reg = [aou_slab[1]-reg_buffer, aou_slab[1]+reg_buffer, aou_slab[2]-reg_buffer, aou_slab[2]+reg_buffer]\n",
    "            \n",
    "            # cut comparison grid down to size of reg and store nodes in an array\n",
    "            work_xyz = pygmt.grd2xyz(grid = search_grid, region = reg, output_type = 'numpy')\n",
    "            \n",
    "            # delete all nodes greater than search_rad km away from aou slab point\n",
    "            del_list = [del_ind for del_ind, array in enumerate(work_xyz) if earth_surf_dist(aou_slab[2], aou_slab[1], array[1], array[0]) > search_rad]\n",
    "            del_list.reverse()\n",
    "            \n",
    "            for del_ind in del_list:\n",
    "                work_xyz = np.delete(work_xyz, del_ind, axis = 0)\n",
    "            \n",
    "            # Count all nodes with z values of 3 or 4 and store as count_slab, corresponding to true positive and false negative (identifies slabs which match\n",
    "            # the tomography model used for comparison)\n",
    "            comp_list = list(work_xyz[:,2])\n",
    "            count_slab = comp_list.count(3) + comp_list.count(4)\n",
    "            \n",
    "            # Find ratio of count_slab/total_nodes, if >25% of nodes correspond to a tomography slab, declare that slab as existing at that depth in our tomography model\n",
    "            slab_ratio = count_slab/len(comp_list)\n",
    "            \n",
    "            if slab_ratio >= 0.25:\n",
    "                working_list.append(d_list[grid_ind])\n",
    "        \n",
    "        # Print out the names and location data of any slabs which don't match tomography at all --> not found in our tomography model\n",
    "        if len(working_list) == 0:\n",
    "            print(aou_name_keys[x], aou_slab)\n",
    "            \n",
    "        # create tuple matching aou slab location (lon,lat,depth) to the depths that slab is found in tomography --> after checking all depths within depth tolerance\n",
    "        work_tup = ([aou_slab[1], aou_slab[2], aou_slab[0]], working_list)\n",
    "        \n",
    "        # Create new dictionary entry matching above tuple with the name of the slab so it can be compared to data in the aou\n",
    "        aou_dict[aou_name_keys[x]] = work_tup\n",
    "    \n",
    "    # return(dictionary created above)\n",
    "    return(aou_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f840285-86fb-4ba1-9742-5ad1531a5684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracks_vs_aou(track_dictionary, aou_dictionary, depth_tolerance, deg_tolerance):\n",
    "    \n",
    "    slab_keys = list(track_dictionary.keys()) # Create a list of the file names in the slab track dictionary\n",
    "    aou_data = list(aou_dictionary.values()) # Create a list of arrays containing aou [[slab locations] [depths it is found on tomography]] \n",
    "    \n",
    "    aou_data = [i[0] for i in aou_data] # aou_data = [lon, lat, depth] --> extract aou slab location data from the aou_data\n",
    "    \n",
    "    aou_depths = [i[2] for i in aou_data] # Create a list of depths at which AOU slab centres are found\n",
    "\n",
    "    spag_files = [] # Initialise list to contain files of slab tracks that match AOU slab centres \n",
    "    \n",
    "    # Iterate over slab files, check the present day location of the slab track, find index of aou_slabs within 100km of the tracked slab\n",
    "    for key in slab_keys:\n",
    "        work_array = track_dictionary[key]\n",
    "        \n",
    "        work_ind = [i for i, depth in enumerate(aou_depths) if abs(depth - work_array[2]) < depth_tolerance]\n",
    "        \n",
    "        # run following code only if there are AOU slabs within 100km depth of tracked slab\n",
    "        if len(work_ind) != 0:\n",
    "            \n",
    "            # store location data for tracked slab\n",
    "            lon1 = work_array[0]\n",
    "            lat1 = work_array[1]\n",
    "            slab_depth = work_array[2]\n",
    "            \n",
    "            # iterate over aou slabs within 100km depth of tracked slab\n",
    "            for ind in work_ind:\n",
    "                \n",
    "                # Store location for aou slab\n",
    "                lon2 = aou_data[ind][0]\n",
    "                lat2 = aou_data[ind][1]\n",
    "                \n",
    "                # Create radius variable inbetween aou slab and tracked slab\n",
    "                mean_rad = ((aou_data[ind][2]+slab_depth)/2)*1000\n",
    "                \n",
    "                # Calculate distance between aou slab and tracked slab\n",
    "                work_dist = earth_surf_dist(lat1, lon1, lat2, lon2, mean_rad)\n",
    "                \n",
    "                work_deg = np.degrees((work_dist*1000)/mean_rad)\n",
    "                \n",
    "                # if tracked slab is within deg_tolerance degrees of aou slab, add tracked slab file to a list\n",
    "                if work_deg < deg_tolerance:\n",
    "                    spag_files.append(key)\n",
    "    \n",
    "    # Return list of slab track files that are within 100km depth and 250km lateral distance of an aou slab               \n",
    "    return(spag_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924eaaba-9a02-43ee-bce4-90c22e3b975c",
   "metadata": {},
   "source": [
    "# Distance Travelled vs Thermal Anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00a24fad-48e8-4550-bea0-867311a15f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_v_temp(tracked_slab_dict, age_grid, sub_stats_array, check_age_conv = True, list_of_slab_files = []):\n",
    "    \n",
    "    if isinstance(tracked_slab_dict, dict) == True:\n",
    "        tracked_slab_files = list(tracked_slab_dict.keys())\n",
    "    else:\n",
    "        tracked_slab_files = list_of_slab_files\n",
    "    \n",
    "    lat_rate_lists, depth_lists, lateral_dist_list, vertical_dist_list = displacement_list('filler', tracked_slab_files)\n",
    "    \n",
    "    if check_age_conv == True:\n",
    "        age_list, mean_age = sample_age_grids(tracked_slab_files, age_grid)\n",
    "        convergence_rates, age_list_unused, migration_rates = sample_conv_rate(tracked_slab_files, sub_stats_array)\n",
    "    \n",
    "    total_dist_list = [(val + vertical_dist_list[i]) for i, val in enumerate(lateral_dist_list)]\n",
    "    \n",
    "    plot_list = []\n",
    "    \n",
    "    #plot3d_list = []\n",
    "    \n",
    "    if isinstance(tracked_slab_dict, dict) == True:\n",
    "        for x, slab_file in enumerate(tracked_slab_files):\n",
    "            \n",
    "            work_array = tracked_slab_dict[slab_file]\n",
    "            \n",
    "            #plot_point = [total_dist_list[x], work_array[-1]]\n",
    "            #\n",
    "            if check_age_conv == True:\n",
    "                plot_point = [lateral_dist_list[x], vertical_dist_list[x], total_dist_list[x], work_array[-1], age_list[x], convergence_rates[x], migration_rates[x]]\n",
    "            else:\n",
    "                plot_point = [lateral_dist_list[x], vertical_dist_list[x], total_dist_list[x], work_array[-1]]\n",
    "            \n",
    "            plot_list.append(plot_point)\n",
    "            \n",
    "            #plot3d_point = [lateral_dist_list[x], vertical_dist_list[x], work_array[-1]]\n",
    "            #plot3d_list.append(plot3d_point)\n",
    "    else:\n",
    "        for x, slab_file in enumerate(tracked_slab_files):\n",
    "            \n",
    "            #plot_point = [total_dist_list[x], work_array[-1]]\n",
    "            #\n",
    "            if check_age_conv == True:\n",
    "                plot_point = [lateral_dist_list[x], vertical_dist_list[x], total_dist_list[x], 0, age_list[x], convergence_rates[x], migration_rates[x]]\n",
    "            else:\n",
    "                plot_point = [lateral_dist_list[x], vertical_dist_list[x], total_dist_list[x], 0]\n",
    "            \n",
    "            plot_list.append(plot_point)\n",
    "            \n",
    "            #plot3d_point = [lateral_dist_list[x], vertical_dist_list[x], work_array[-1]]\n",
    "            #plot3d_list.append(plot3d_point)\n",
    "    \n",
    "    plot_array = np.array(plot_list)\n",
    "    \n",
    "    #plot3d_array = np.array(plot3d_list)\n",
    "    \n",
    "    return(plot_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7bcf99-8fab-45e4-a29a-94797f0635dd",
   "metadata": {},
   "source": [
    "# Success Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e86656ac-c708-4f1e-84b6-6eb0cda6c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slab dicts should be in the order (slab_match, slab_prob, thermal_equilibrium_match, thermal_equilibrium_prob)\n",
    "\n",
    "# list_of_list_of_dicts = [list_dicts_40ma, list_dicts_80ma, list_dicts_120ma, etc...]\n",
    "# list_dicts_40ma = [gcm30_slab_match_dict_40ma, gcm30_slab_prob_dict_40ma, gcm30_melt_match_dict_40ma, gcm30_melt_prob_dict_40ma]\n",
    "\n",
    "def all_ages_success_rate(list_of_list_of_dicts):\n",
    "    \n",
    "    match_dicts = [(len(list_dicts[0]) + len(list_dicts[2])) for list_dicts in list_of_list_of_dicts]\n",
    "    \n",
    "    prob_dicts = [(len(list_dicts[1]) + len(list_dicts[3])) for list_dicts in list_of_list_of_dicts]\n",
    "    \n",
    "    tomo_success_rate = sum(match_dicts)/(sum(match_dicts) + sum(prob_dicts))\n",
    "    \n",
    "    \n",
    "    tp_slab_match = [len(list_dicts[0]) for list_dicts in list_of_list_of_dicts]\n",
    "    \n",
    "    tp_slab_prob = [(len(list_dicts[1]) + len(list_dicts[2]) + len(list_dicts[3])) for list_dicts in list_of_list_of_dicts]\n",
    "    \n",
    "    track_match_tp_rate = sum(tp_slab_match)/(sum(tp_slab_match) + sum(tp_slab_prob))\n",
    "    \n",
    "    tomo_success_rate = tomo_success_rate*100\n",
    "    \n",
    "    track_match_tp_rate = track_match_tp_rate*100\n",
    "    \n",
    "    return(tomo_success_rate, track_match_tp_rate)\n",
    "\n",
    "\n",
    "def success_rate_per_age(list_of_list_of_dicts):\n",
    "    \n",
    "    tomo_success_rates = []\n",
    "    \n",
    "    model_success_rates = []\n",
    "    \n",
    "    for at_age_list in list_of_list_of_dicts:\n",
    "        \n",
    "        tp_num = len(at_age_list[0])\n",
    "        fn_num = len(at_age_list[2])\n",
    "        \n",
    "        prob_num = len(at_age_list[1])+len(at_age_list[3])\n",
    "        \n",
    "        tomo_success = (tp_num+fn_num)/(tp_num+fn_num+prob_num)\n",
    "        tomo_success_rates.append(tomo_success)\n",
    "        \n",
    "        model_success = tp_num/(tp_num+fn_num+prob_num)\n",
    "        model_success_rates.append(model_success)\n",
    "        \n",
    "    tomo_success_rates = [(i*100) for i in tomo_success_rates]\n",
    "    \n",
    "    model_success_rates = [(i*100) for i in model_success_rates]\n",
    "        \n",
    "    # return(list showing success rate for each time period)\n",
    "    return(tomo_success_rates, model_success_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe93ab-d59d-4643-a27c-9e296f866b4b",
   "metadata": {},
   "source": [
    "# Sample Age Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9e1248a-5183-4e0d-8ce0-4b4b5ad8e186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_age_grids_defunct(slab_initials_list, age_grid, test_index = None, input_type = 'list_of_lists'):\n",
    "    \n",
    "    if test_index != None and input_type == 'list_of_lists':\n",
    "        \n",
    "        working_array = slab_initials_list[test_index]\n",
    "        \n",
    "        df_subduction_age = pygmt.grdtrack(grid = age_grid, points = working_array, z_only = True)\n",
    "        \n",
    "        work_list = df_subduction_age.values.tolist()\n",
    "        \n",
    "        age_list = [age for sublist in work_list for age in sublist]\n",
    "        \n",
    "        mean_age = sum(age_list)/len(age_list)\n",
    "        \n",
    "        return(age_list, mean_age)\n",
    "    \n",
    "    elif input_type != 'list_of_lists':\n",
    "        \n",
    "        slab_initials = plot_initial_slabs(slab_initials_list)\n",
    "        \n",
    "        working_array = slab_initials\n",
    "        \n",
    "        df_subduction_age = pygmt.grdtrack(grid = age_grid, points = working_array, z_only = True)\n",
    "        \n",
    "        work_list = df_subduction_age.values.tolist()\n",
    "        \n",
    "        age_list = [age for sublist in work_list for age in sublist]\n",
    "        \n",
    "        mean_age = sum(age_list)/len(age_list)\n",
    "        \n",
    "        return(age_list, mean_age)\n",
    "    \n",
    "    else:\n",
    "        list_of_age_lists = []\n",
    "        \n",
    "        list_of_mean_ages = []\n",
    "        \n",
    "        for slab_list in slab_initials_list:\n",
    "            \n",
    "            working_array = slab_list\n",
    "        \n",
    "            df_subduction_age = pygmt.grdtrack(grid = age_grid, points = working_array, z_only = True)\n",
    "            \n",
    "            work_list = df_subduction_age.values.tolist()\n",
    "            \n",
    "            age_list = [age for sublist in work_list for age in sublist]\n",
    "            \n",
    "            mean_age = sum(age_list)/len(age_list)\n",
    "            \n",
    "            list_of_age_lists.append(age_list)\n",
    "            list_of_mean_ages.append(mean_age)\n",
    "            \n",
    "        return(list_of_age_lists, list_of_mean_ages)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a81ef6e9-7298-45a7-9c51-6f16056dba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_age_grids(age_slab_files, age_grid):\n",
    "    \n",
    "    work_initials = plot_initial_slabs(age_slab_files)\n",
    "    \n",
    "    working_array = work_initials[0]\n",
    "    \n",
    "    df_subduction_age = pygmt.grdtrack(grid = age_grid, points = working_array, z_only = True)\n",
    "    #, nodata = '200'\n",
    "    \n",
    "    work_list = df_subduction_age.values.tolist()\n",
    "    \n",
    "    age_list = [age for sublist in work_list for age in sublist]\n",
    "    \n",
    "    mean_age = sum(age_list)/len(age_list)\n",
    "    \n",
    "    return(age_list, mean_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c906a3d5-dd44-473c-ae09-22c29b918a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_age_grids(agegrid_dir, output_dir, above_list):\n",
    "    \n",
    "    age_grids = []\n",
    "    \n",
    "    for file in sorted(os.listdir(agegrid_dir)):\n",
    "        fn = os.path.join(agegrid_dir, file)\n",
    "        \n",
    "        age_grids.append(fn)\n",
    "    \n",
    "    for in_grid in age_grids:\n",
    "        \n",
    "        fn = (in_grid.split('/'))[-1]\n",
    "        \n",
    "        output = output_dir + '/' + fn\n",
    "        \n",
    "        pygmt.grdclip(grid = in_grid, outgrid = output, region = 'd', above = above_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a3f4d-5f00-4fb5-96ce-306f3316096c",
   "metadata": {},
   "source": [
    "# Sample Convergence Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c18dee9-0515-4cb9-8139-6dd453094974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_conv_rate(tracked_slab_dict, sub_stats_array):\n",
    "    \n",
    "    if isinstance(tracked_slab_dict, dict) == True:\n",
    "        track_files = list(tracked_slab_dict.keys())\n",
    "    else:\n",
    "        track_files = tracked_slab_dict\n",
    "    work_initials = plot_initial_slabs(track_files)[0]\n",
    "    \n",
    "    convergence_rates = []\n",
    "    seafloor_ages = []\n",
    "    migration_rates = []\n",
    "    \n",
    "    test_subs = []\n",
    "    test_tracks = []\n",
    "    \n",
    "    for array in work_initials:\n",
    "        lon = array[0]\n",
    "        lat = array[1]\n",
    "        test_tracks.append([lon,lat])\n",
    "        \n",
    "        check_lon = 5\n",
    "        work_stats = sub_stats_array[abs(lon - sub_stats_array[:,0]) < check_lon]\n",
    "        while len(work_stats) == 0:\n",
    "            check_lon = check_lon+2.5\n",
    "            work_stats = sub_stats_array[abs(lon - sub_stats_array[:,0]) < check_lon]\n",
    "        \n",
    "        check_lat = 5\n",
    "        final_stats = work_stats[abs(lat - work_stats[:,1]) < check_lat]\n",
    "        while len(final_stats) == 0:\n",
    "            check_lat = check_lat+2.5\n",
    "            final_stats = work_stats[abs(lat - work_stats[:,1]) < check_lat]\n",
    "        \n",
    "        if len(final_stats) == 1:\n",
    "            convergence_rates.append(final_stats[0][2])\n",
    "            seafloor_ages.append(final_stats[0][4])\n",
    "            test_subs.append([final_stats[0][0], final_stats[0][1]])\n",
    "            migration_rates.append(final_stats[0][5])\n",
    "            \n",
    "        else:\n",
    "            work_dists = []\n",
    "            for stats in final_stats:\n",
    "                lon2 = stats[0]\n",
    "                lat2 = stats[1]\n",
    "            \n",
    "                dist = earth_surf_dist(lat, lon, lat2, lon2)\n",
    "                work_dists.append(dist)\n",
    "                \n",
    "            ind = work_dists.index(max(work_dists))\n",
    "            convergence_rates.append(final_stats[ind,2])\n",
    "            seafloor_ages.append(final_stats[ind,4])\n",
    "            test_subs.append([final_stats[ind][0], final_stats[ind][1]])\n",
    "            migration_rates.append(final_stats[ind,5])\n",
    "            \n",
    "    # test_subs, test_tracks        \n",
    "    return(convergence_rates, seafloor_ages, migration_rates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pygmt]",
   "language": "python",
   "name": "conda-env-pygmt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
